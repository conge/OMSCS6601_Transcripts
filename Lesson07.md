# Lesson 7

## Title 01 - Introduction to Machine Learning 
Thad, what's the difference between pattern recognition and machine learning? &gt;&gt; In practice, not much. Often the terms are used interchangeably. In theory, though, pattern recognition is not necessarily imply learning. &gt;&gt; Okay, how about data mining &gt;&gt; Again, the difference can be subtle. Often data mining focuses on exploring a large data set and trying to explain or better utilize portions of it. Often the same techniques are used in pan recognition, machine learning, and data mining. We want to deduce these sorts of techniques in this section and try to provide an intuition for when to use them. &gt;&gt; What sorts of problems can these algorithms address? Typical tasks include optical character recognition, face recognition, text retrieval, handwriting recognition, speech recognition, activity recognition, web search, and spam filtering. But toolkits like and HTK have allowed more and more people to try machine learning on a large variety of problems. &gt;&gt; Most of the problems you mentioned are for supervising where we have labeled examples from which to learn. What about unsupervised learning where we don't have labels? &gt;&gt; We'll cover some of those algorithms as well in this lesson. In fact, much of my group's current research is on creating new unsupervised algorithms which help researchers discover repeated patterns in large databases. For example, my group is working with the wild dolphin project to try to discover the fundamental units of vocalization used by Atlantic spotted dolphins. We'll mention some of that in the next lesson called parent recognition through time. In this lesson, we'll focus more on situations where the data represents a static image, or discrete moment in time. To orient everyone, let's start with a challenge question.

## Title 02 - Challenge Question 
Given the following data about the on campus AI class from last semester, construct a compact decision tree using Greedy's search and information gain that predicts student's grades this semester. Pick the most appropriate decision tree from these choices.

## Title 03 - Challenge Question Solution 
The answer is C. With the first attribute of Does Projects, the tree classifies 50% of it's training examples, Attends Class, then separates the other two cases into B and C. Tree D works, but it's not nearly as compact. B is compact, but actually classifies examples from its training set incorrectly. After the two A grades are separated by the Attends Class attribute, the tree is trying to differentiate the last two examples using Reads Book. Both remaining training examples are classified as B grades as the attribute Reads Book does not differentiate them. A is pretty similar to C in that it's compact, but construction was not sufficiently greedy. The first attribute only classifies one training example definitively whereas the tree in C classified two of the training examples on the first question. Decision trees are an understandable and easy to learn method of classification, however they have trouble approximating certain functions like parody. Machine learning requires understanding a variety of methods and choosing ones that will most likely solve a given classification problem. In the following lesson, we hope to provide some intuition to some of the more popular machine learning methods.

## Title 04 - k-Nearest Neighbors 
Let's start a discussion on machine learning with K nearest neighbors. &gt;&gt; That sounds like a good idea. It is one of the simplest techniques to understand, and shows the challenges of choosing the right parameters for an algorithm. &gt;&gt; Right. Suppose we have some two dimensional training data, with two classes. Plus, and O. &gt;&gt; What do the pluses and O's represent? &gt;&gt; It could be anything. You're the one who tells stories in class all the time. &gt;&gt; Okay. Let's pretend I am trying to model two types of sharks by the length and width of their bite marks. &gt;&gt; Hold on. This is really specific. Where'd it come from? &gt;&gt; I was on vacation in Hawaii when I wrote this section and they were having a record number of shark attacks at the time. &gt;&gt; I know you make up these examples on the fly during class, but this one is strange even for you. &gt;&gt; Just go with it. The x-axis is the width of the wound. The y-axis is the length. &gt;&gt; Okay. &gt;&gt; And we are trying to determine is from the cookiecutter shark, which is named for the roundish bite marks it leaves in its prey. &gt;&gt; Now you're making things up again. &gt;&gt; No, no, really. Go look it up. So the pluses represent wounds we know are made from the cookiecutter sharks. &gt;&gt; And O's are from great whites? &gt;&gt; No, they're too big. But for the sake of illustration, let's let the O's be all the other bite marks we observed. &gt;&gt; Are we talking bites like on humans on a beach? &gt;&gt; No, bites on cetaceans like whales. &gt;&gt; Somehow that did not make me feel better about this example. &gt;&gt; We're trying to figure out if there has been a surge in the cookie cutter shark population, by looking at the number and types of wounds on whales because the cookie cutter sharks are rare to see, and only come out to feed at night. &gt;&gt; That actually sounds like a real problem. &gt;&gt; Only somewhat. I needed a good story to match an old data plot I had, so I did some research and came up with this one. &gt;&gt; Your mind is an interesting place to live in. &gt;&gt; I'll take that as a compliment. At the very least, this example shows the incredible variety of problems one might use machine learning for. &gt;&gt; Okay. We'll run it. Suppose I have a new bite mark I've observed on a whale and it fits into the data here. Is it a cookie cutter bite? &gt;&gt; I'd say no. It looks like a normal bite because all the other examples in the region around it are Os, which represent normal bites. &gt;&gt; Now how about this one? &gt;&gt; Looks like a cookie cutter bite to me because all the other bites around it are pluses. &gt;&gt; Care to make an algorithm around that idea? &gt;&gt; Sure. Find the nearest example in the training data set to the unknown example, and use whatever label the training example has. &gt;&gt; Okay, that algorithm will be called one nearest neighbor. &gt;&gt; But isn't the algorithm called K nearest neighbor? What's the K for? &gt;&gt; Well, sometimes you use K closest training set examples to classify your unknown example. Why would you do that? &gt;&gt; Take a look at this example. Is it a cookie cutter bite or a normal shark bite. &gt;&gt; Well the nearest neighbor to it is cookie cutter, but the next nearest are all normal shark bites. &gt;&gt; So which is it? &gt;&gt; I'd go with normal. &gt;&gt; How many nearest neighbors have you used here? &gt;&gt; Well in this case, I used K equals three and just label the unknown example with whatever the majority label was from the training examples. But how will I figure out what k is in general? &gt;&gt; Knowing you, you'd just try lots of values of k and pick the best. &gt;&gt; Precisely, but let's make that process a bit more formal.

## Title 05 - Cross Validation 
Cross validation is a testing technique that can help us tune the parameters of whichever technique we are using. It can also help us estimate what our performance is going to be on a problem, give us an idea of how hard a problem is and help to avoid a problem called over fitting. &gt;&gt; Okay, let's get started. What should we do first? &gt;&gt; First let's suppose we have 100 examples of training. We're going to make an important assumption here. But the data we've collected for training is a good representation of the problem we're trying to solve. And also, that the data spans the space of the unknown examples we might encounter. &gt;&gt; Why wouldn't that be the case? &gt;&gt; For many, many reasons. Let's take our shark bite example. Perhaps we don't have enough examples to really capture all the types of cookie cutter shark bites. Perhaps we only looked at bite marks on whales for training, but then we later decide to look at bite marks on spinner dolphins. Maybe only smaller sharks attack dolphins, or maybe we only see the whales during the winter, when they are migrating through Hawaii and the most mature sharks attack. Whereas spinner dolphin attacks happen year round and are smaller and more oval. I"m making up these specific situations, but these types of problems are real. Often a researcher spend a lot of effort making a recognizer, only to discover the data does not really represent the problem. Or that they're rare, but important examples that don't happen to be in the training set. &gt;&gt; Okay, let's assume the data we have does represent the problem adequately. &gt;&gt; Okay, let's represent those 100 examples with this gray box. We're going to choose 10% of those examples randomly and reserve them for the future. We're going to take the rest of the data and divide that into a training and test set. &gt;&gt; Okay, so I'll suggest that we take our remaining 90 examples and use 20% for testing. &gt;&gt; That would be 18 of the remaining 90 examples. &gt;&gt; And then the rest for training. &gt;&gt; That's a fine idea but it's important to choose that 80% and 20% randomly and not to mix the two. &gt;&gt; Why is that? &gt;&gt; Well, having some of our training set and our testing set will give the algorithm an unfair advantage. &gt;&gt; Okay, I see. So you use all the day for training and then all of it for testing. With Kenny's nearest neighbors and k=1, I can get 100% accuracy. &gt;&gt; That's exactly right. That situation is a classic example of over fitting. With k=1 and training and testing being the same data, it's easy to get a high recognition rate. Basically, for each test datum, the training datum that is closest is itself. But then, when we get data that was not in the training set, result might not generalize well to that new data. This tension between over fitting and generalization is one of the main challenges in machine learning. &gt;&gt; Okay, so let's go back to my suggestion. We'll use 80% for training. &gt;&gt; And 20% for an independent test set. &gt;&gt; An independent test set? &gt;&gt; It's just a buzz word that none of the trained data is in the test data. Machine learning researchers look for those magic words when reviewing papers. When writing up a result, it's always good to explicitly say you chose a randomly chosen independent test set. &gt;&gt; Hold on, why random? &gt;&gt; To avoid bias. Imagine that our bite marks on our whales were tabulated over the winter season. During that time, the cookie cutter sharks were growing. If we just chose the first 80% of the data for training, and the last 20% for tests, we might bias our training set against ourselves. Because the biggest circular bite marks are not included in the training set. &gt;&gt; That makes sense. And I guess there are lots of potential biases in collecting real data. &gt;&gt; Correct, there are trends in almost every real data set, and to combat that, we use randomness. &gt;&gt; Okay, so we selected our 80% 20% randomly. Next, for each example in the 20% independent test set, we compare it to the training data. And since we're using k=3, we look for the three closest neighbors. We label each test example with the majority label. &gt;&gt; And then compare it to the actual known value. &gt;&gt; We then calculate the percentage correct and report that as our accuracy. &gt;&gt; But we're not done here. &gt;&gt; We're not? &gt;&gt; No, suppose I got a result of 97%. I could have just gotten lucky in my selection of test cases. Remember, we started out with 100 examples. We reserved ten for future use, and choose 72 for training and 18 for tests. Those 18 could have happened to be the easy cases. &gt;&gt; Don't tell me, we're going to solve this problem with more iterations? &gt;&gt; Yep, we're going to divide the 90 examples randomly again into 80% training and 20% test. And we're going to calculate our accuracy on that set. Then we'll do it again and again. &gt;&gt; And average the results. How many times should we do it? &gt;&gt; Well, whatever makes sense for the amount of computer time it takes and the amount of data available. In this case, since we have 90 choose 18 possibilities. &gt;&gt; Which is over ten to the 18. &gt;&gt; The chance that we get a lot of repeated divisions of the database is small. So I would use something like 100 iterations and then average the result. &gt;&gt; You would then report we had an average 93% accuracy using 100 for cross validation. Using randomly chosen independent test subs. &gt;&gt; That's right, you must be looking ahead in the script. &gt;&gt; No, no, that's my line. See it says it right there. &gt;&gt; Okay, yeah you're right. &gt;&gt; Let's talk about that reserve 10% of the data. Why did we do that? &gt;&gt; Okay, suppose we have a situation where we didn't know what value for k worked well for our problem. &gt;&gt; Well, we could just start with k equals one, do 100 fold cross validation, then k equals two, do cross validation again. And keep going until we get to the largest value of k, which is reasonable to test, and then choose the best. &gt;&gt; Yep, but then we've over fit our algorithm's parameters to the data. &gt;&gt; What do you mean? &gt;&gt; Tweaking an algorithm's parameters excessively can lead to overfitting, to that particular set of data. How we combat this problem, is to use our 90 examples, treating our parameters and algorithms, until we get the best results we think are reasonable. And then doing one more test, the test on that reserve 10% of the data. &gt;&gt; So we only do that test at the end, and then that result is what we hope will be representative of how our will work in real life. &gt;&gt; That's the idea. And if the data set is big enough, and represents a problem well enough, and we don't get unlucky with randomization, it should be okay. &gt;&gt; Is this sort of testing how those machines in competitions work? &gt;&gt; Yep, when DARPA was funding face recognition algorithm research, they reflect a large data set. Give some of it to the researchers to tune their algorithms. Then, use the reserve testing set to see how well the algorithms actually worked. These competitions often led to continued funding in the millions of dollars. &gt;&gt; Or not. &gt;&gt; Or not. The procedures were taken very seriously. Today, we see similar competitions at conferences and on websites. Some are for fun, others are for real money. Going to caggle.com and chlearn.org will show some examples of current competitions. Probably one of the most famous, though, is the Netflix prize. &gt;&gt; I remember that one. They paid $1 million dollars for the algorithm that could improve the predictions of whether or not their users would like a movie by 10%. &gt;&gt; And they used a strategy similar to what we are talking about to do the testing. &gt;&gt; But Netflix has millions of examples in their data set. What if you only have a few? &gt;&gt; What, like 10 examples? &gt;&gt; Sure. &gt;&gt; Well, then you do a leave one out strategy. &gt;&gt; You mean train on nine and test on the tenth for all ten combinations of the possible training testing sets? Yep, in papers you'll sometimes see this abbreviated as LOOCV, or Leave One Out, Cross Validation. &gt;&gt; I suppose another way to do it is to leave a couple out. &gt;&gt; Yep, training on eight and testing on two allows ten choose two possibilities. &gt;&gt; Which equals 45. &gt;&gt; How did you? &gt;&gt; The script, remember? &gt;&gt; Yeah, hm. &gt;&gt; Does that sort of testing lead to publishable results? &gt;&gt; It does, if the data are rare or each example's very complicated. &gt;&gt; Okay, what about the amount of variation between folds of cross validation? Does that tell us anything? &gt;&gt; Yep, it can tell us how sensitive the system is to small changes. I don't like to depend on a recognizer that shows a lot of variation in results between folds. It could mean the algorithm is too sensitive to outliers in the data, or that I don't understand all the variables that are affecting the data. &gt;&gt; It seems to me that we can also use cross validation to determine how sensitive an algorithm is to its parameters. Yep, that's a good point. For example, if we are using k nearest neighbors, and we see a huge change in average accuracy between k equals ten and k equals 11, there is probably something fishy going on. The same can be said for any algorithm. We have to choose a parameter for the algorithm to work. We want to know how bad things can get if we are a little off. Stability to initial parameters and initial conditions is something we like to see in any algorithm we are using. &gt;&gt; So we've talked about keeping the training and testing data separate. Is there any time when training on testing data is allowed? &gt;&gt; Personally, I like to do it to see how hard a problem is, how prone to over fitting my algorithm is. I'll compare my results on testing on training to a fair test and see how much difference there is between them. If there is a lot, I know that the algorithm is over fitting or that I don't have enough training data yet. Or if I cant even get good recognition results, even on the training data, I know the problem is hard or I am using the wrong algorithm. &gt;&gt; Wow, we spent a lot of time on cross validation. Perhaps it's time for a quiz. &gt;&gt; That sounds like a good idea.

## Title 06 - Cross Validation Quiz 
Suppose you are given a top secret project by the government. Your mission, should you choose to accept it, is to a design system to identify when something important is happening at the Kremlin by observing the automobile traffic outside. Once finished, your system's accuracy will be compared against other means as new satellite data comes in. However, to get you started the government gives you 100 days worth of observations, which are annotated as important or unimportant. Given these choices, which seems most reasonable to create the best system possible?

## Title 07 - Cross Validation Quiz Solution 
The last two answers train the system on all 100 days of data. They are both going to result in overfitting, even though the last answer does have a more involved test strategy. The first answer is better in that it has an independent test set. However, we're not repeating the process, so it's possible to just get lucky or unlucky in the split of the data. Therefore, this is the answer. We randomly choose our training set and our test set. We also withheld a final test set, which will represent the final performance of the system. And we've repeated the process until we have good results.

## Title 08 - The Gaussian Distribution  
We're going to be using the Gaussian distribution for a lot of our discussions on machine learning. &gt;&gt; What does it look like? &gt;&gt; It's the bell curve we often talk about in grading. &gt;&gt; You mean the one we use when we calculate the mean and standard deviation of grades in class? &gt;&gt; Yep. &gt;&gt; Great, what's it's formula? &gt;&gt; It's an exponential. Mu is the mean and sigma squared is the variance. &gt;&gt; And the standard deviation is just the square root of the variance, so there's just sigma here. And the standard deviation tells us how fat the Gaussian is, right? &gt;&gt; Right, and there are some other nice properties of Gaussians. &gt;&gt; Like what? &gt;&gt; Well 68% of the probability mass is within 1 standard deviation of the mean. And 95% is within 2 standard deviations. 99.7% is within 3 standard deviations. &gt;&gt; Now I get why teachers grade on the curve. If a student is within 1 standard deviation of average, they get a C, which should account for most of the class. &gt;&gt; 68% to be precise. &gt;&gt; They account for some variance in randomness in performance in grading above and below the mean. If the student achieves between 1 and 2 sigmas above the mean, they get a B. And above 2 sigmas they get an A. &gt;&gt; But that would mean that only 2.5% of students would get an A. &gt;&gt; And 13.5% would get a D. Hm, that seems a bit harsh. &gt;&gt; Right, many grad courses now grade so that the A B line is at the mean. And to get a C means you had to be 2 standard deviations below it. &gt;&gt; Hm, so how are we grading this class again? &gt;&gt; I think that's a topic for later. The main point we're trying to get acquainted with here is the properties of the distribution. &gt;&gt; But why do we use the Gaussian for grading anyways?

## Title 09 - Central Limit Theorem 
Because of something called the central limit theorem. &gt;&gt; What's that? &gt;&gt; Well, very loosely, it says that if you have enough independent random variables, the sum of them will form the normal distribution which is another name for the Gaussian. &gt;&gt; Okay, let me see if I get this right. If we have enough factors influencing the student's grade like how smart they are, how good the lessons are, if they got enough sleep, how all the assignments are written, etc, the grades will approach the shape of a Gaussian? &gt;&gt; Well in theory, if there are enough factors and we have enough students. In practice those assumptions don't always hold for grading, but for many other real-world situations it's a good approximation.

## Title 10 - Grasshoppers Vs Katydids 
Okay let's do a pattern recognition example that uses the Gaussian distribution. I found this one from my colleague, Evan Keyo's work on computational entomology. &gt;&gt; What? &gt;&gt; Studying insects using computer science. Evan likes to apply data mining techniques to interesting domains and he creates some good examples. &gt;&gt; That's something that I never would have thought to do but I'm not really a fan of bugs. Here we are trying to classify insects as grasshoppers or katydids by the abdomen length or antenna length. &gt;&gt; That's not so bad. &gt;&gt; Blue represents the grasshoppers and red represents the katydids. Let's put up a lot of data and show the plot histograms of the antenna lengths by projecting the data on the y axis. &gt;&gt; And that way we can see that the data is forming a Goshen. &gt;&gt; Correct, instead of using histograms. We're going to fit a normal distribution to the data. &gt;&gt; So you mean you're going to calculate the mean and standard deviation of the grasshopper data and use that to plot a Gaussian with the same median and standard deviation. &gt;&gt; And we will do the same thing for the katydid data as well. Now suppose I have an insect with an antenna length of three units. Is it more likely a grasshopper or a katydid? Probably a grasshopper. &gt;&gt; Correct. Because the height of the blue grasshopper curve at three is much higher than the red katydid curve. &gt;&gt; Can we put some probabilities on our guess? &gt;&gt; Yep pretty easily. What we are trying to do is calculate the probability that we are seeing a specific case c sub j given that we have seen data d. &gt;&gt; Hold on, all you need to do is measure the heights of the curves at x equal to 3 and normalize them by the sum? &gt;&gt; Yep, and since we already have the equation of the Gaussian from simply calculating the mean and standard deviation of the data- &gt;&gt; It becomes pretty easy. Just plug in the x value, get out the y value and do some simple division. Wow, that's easy. &gt;&gt; Because the central limit theorem, a lot of data tends towards being Gaussian, so this trick works for a surprising number of problems.

## Title 11 - Gaussians Quiz 
Here we have the distributions for grasshopper in blue and katydid in red. Take a look at the attached Excel sheet. Use that to calculate the mean and standard deviation of each distribution. Then, can use the equation for the Gaussian to tell us the probability that we have an antenna length of 7.

## Title 12 - Gaussians Quiz Solution 
Here's the answer. As you can see for an antenna length of 7, it's much more likely that our insect is a katydid.

## Title 13 - Decision Boundaries 
I guess the break-even point is where the curves cross. &gt;&gt; What do you mean? &gt;&gt; Well, at an antenna length of 5, there's equal probability that an insect is a grasshopper or a katydid. &gt;&gt; Right. That point is called a decision boundary. &gt;&gt; Meaning that everything to the right of the decision boundary is going to be guessed to be a grasshopper, and everything to the left of it is going to be predicted to be a katydid. &gt;&gt; Hey, I just noticed something. Our variances are the same in both classes. What if they are different? &gt;&gt; You can still figure the decision boundary just by looking at where the Gaussians cross. Here's an example of Gaussians with different variances. &gt;&gt; That's cool. &gt;&gt; One thing to notice with two classes of different variances is that we can get decision boundaries where we have two thresholds instead of one. &gt;&gt; How can that happen? &gt;&gt; Here's an example. Imagine that both our classes have the same mean, but different variances. The one with the lower standard deviation is skinnier and rises above the fatter one in the middle, but on either side of the middle, the fatter one wins. &gt;&gt; Neat, but what if one class is more probable than the other? &gt;&gt; You mean like if there are two times as many grasshoppers as katydids? &gt;&gt; Sure. &gt;&gt; Well, so far we've assumed equal probability of the classes, and the sum of the area under each Gaussian had to be 1. But we could change that so that the area under the Gaussian for the grasshoppers is 2. In this case, the boundaries that indicate if an insect is a katydid get pulled in towards the mean a bit more. &gt;&gt; I'm glad we talked about this issue. We've mostly been assuming the prior probabilities of the class have been equal, but it can have a big effect on creating a classifier when once class is much more likely than the other. &gt;&gt; Perhaps we should have a quiz on this idea.

## Title 14 - Recognition Quiz 
Let's say I have a set of data consisting of 10% positive examples and 90% negative examples. A student provides me with a recognizer that works 90% of the time. Should I believe that they have made a working recognizer?

## Title 15 - Recognition Quiz Solution 
The answer is no. If the student's classifier just returned negative 100% of the time, it would get 90% accuracy. We hope that the recognizer could do much better than that. When evaluating a recognizer, we must ask, what percentage of the database is a large class, and then compare the accuracy of our recognizer to that. If we expect 50% positive and 50% negative examples then 90% accuracy is actually impressive. But if we expect to see negatives 97% of the time then a 90% accuracy recognizer is doing very poorly.

## Title 16 - Decision Boundaries in Higher Dimensions 
Okay, I'm going to keep asking questions because I like these pretty graphics. &gt;&gt; Sure, go ahead. &gt;&gt; What if we have 2D data like how we started? Having additional dimensions might help separate the different classes of data better. &gt;&gt; Well assuming that we are modelling the distributions with 2D Gaussians, then the decision boundaries will be a conic section. &gt;&gt; You mean like lines, parabolas, hyperbolas, ellipses, or circles? &gt;&gt; Yep, and when using three dimensional Gaussian distributions, we get Bayes decision boundaries that are two dimensional hyperquadratics. &gt;&gt; Those are quite pretty.

## Title 17 - Error 
We can also use our graphs to understand how much error we are going to have as a result of setting a decision boundary at a certain point. &gt;&gt; How so? &gt;&gt; Let's use an example from Keogh again. He is using backscatter from a laser to determine the frequency of a mosquito's wing's beating. Based on that frequency, he determined which types of mosquitos are in that area. &gt;&gt; Wait, really? &gt;&gt; Actually, one of the first uses of the principles of passive RFID was for this sort of problem. Back when the Mediterranean fruit fly was embedding California, scientists coated some fruit flies' wings with silver nitride, sterilized them and released them to see which orchards they would migrate to and how fast they'd moved. Using a radar gun, they could capture the reflected radiation. And since the fruit fly's wings were the only things oscillating around the frequency of their beating wings, they could track them really well. Given Keogh's work, it seems they can do similar things with lasers now. &gt;&gt; Without coating everything in silver? &gt;&gt; Presumably. So back to the problem. We know that our decision boundary between these two species of mosquitoes is 517 hertz. What is our expected error? &gt;&gt; Well, to the left of the decision boundary we are going to have some insights from the right class, mis-classified as left class. We can calculate that just by integrating the right Gaussian from negative infinity to 517. &gt;&gt; Or we could use tables to look up the area under the curve for a Gaussian up to that number of standard deviations to the left of the mean. Statistics books often have these sorts of tables. In this case, the area under the curve up that point is 12.2% of the whole. &gt;&gt; So we can do the same thing to the other side as well. &gt;&gt; Yep. Given our decision boundary, everything under the red curve is going to be misclassified to the right of 517 hertz. That area is 8.02%. &gt;&gt; So that means our classifier is going to have an error of 20.22%? &gt;&gt; Yep and that amount of error is the best we can do given this feature and Gaussian data. But what if misclassifying one type of mosquito is much worse then misclassifying the other? &gt;&gt; When would that happen? &gt;&gt; Well as I was working on this lesson in Hawaii they were having an outbreak of Dengue Fever. They were spraying to reduce the mosquito population. However, only a few species of mosquito carry dengue. Suppose the Gaussian on the left represents the species that carries dengue, but the one on the right is a harmless mosquito. &gt;&gt; We live in the south, there's no such thing as a harmless mosquito. &gt;&gt; Point taken, so to speak. &gt;&gt; Let's just say that the right distribution is a mosquito that doesn't carry dengue. &gt;&gt; Okay. So if we are using our lasers to determine if a particular garden needs spraying, we want to err on the side of caution. &gt;&gt; So we can move our decision boundary to the right, so there's a minimum chance that we will miss a dengue carrying mosquito. &gt;&gt; Correct, and now the question is, how much do we move the boundary? Let's say it is ten times worse to misclassify a mosquito from the left dengue carrying class, than from the right, less dangerous class. &gt;&gt; So we move the boundary to the right. &gt;&gt; So that the amount of error contributed by the area under the right curve is 10x more than the error contributed by the area under the left curve. &gt;&gt; So basically we can weight our decision based on whatever areas we want to avoid. We might cause more error, but those errors are less serious. &gt;&gt; Correct.

## Title 18 - Bayes Classifier 
Perhaps we should be more formal about the Bayesian Classifiers as well? &gt;&gt; If we must. I like to learn the intuition first, and then stare at the math in the book until I really get it. &gt;&gt; This stuff isn't hard, it's just Bayes Rule again. Let's do a quick review of Bayes Rule. Take a look at the formula. We are calculating the probability of class cj, given that we have data point d. Which is equal to p(D/Cj). &gt;&gt; Which is the probability of generating instance of d being in class c sub j, &gt;&gt; Times p(Cj) &gt;&gt; Which is the probability of the occurrence of class c sub j. &gt;&gt; Divided by P of d. &gt;&gt; Which is the probability of instance d occurring. The hard part seems to be that p of d given c sub j. &gt;&gt; How so &gt;&gt; Well, if I have a piece of data and I'm trying to find out if it belongs to class one or two &gt;&gt; Or classes three, four, five, etc. &gt;&gt; Good point, a number of classes. The divisor p of d is going to be the same across all the equations. We probably know the prior probability of each of the classes p of c sub j simply by counting the number of each in our training data. If we're lucky they're all equal. &gt;&gt; And they affect each equation the same. &gt;&gt; Which means the important part remains is the p of d given c sub j. &gt;&gt; Which we are modeling with the curves above. &gt;&gt; Or we can just use simple counting to determine. &gt;&gt; Actually, that's not a bad idea. Let's do a simple one d example where we estimate our distributions with simple counting to reinforce the the basic concept of what is going on before we move on.

## Title 19 - Bayes Rule Quiz 
Assume we have two classes, male and female. We have a person whose gender we do not know named Drew. Given the data base of names and genders, can you tell us whether it is more likely for Drew to be male or female?

## Title 20 - Bayes Rule Quiz Solution 
The answer is Female. Let's go through the math. We need to find out which is greater, the probability of male, given Drew, or the probability of female, given Drew. We can use a standard Bayes rule formula. For example, for male it would be probability of Drew, given male, times probability of male all over the probability of Drew. Let's do male first. There are three males in our database and only one is named Drew, so we have a 1/3 to start. Then for the probability of male, we have 3 males out of the 8 people in our database. The in the denominator we have the probability of Drew. We have three Drews in our database, so that's, again, 3/8. Next is female. We have 5 females in the database, and 2 of them are named Drew. That gives us our 2/5th here. And then 5 females out of our 8 person database. The denominator is the same. We have 3 Drews in our 8 person database. So we really only need to compare the numerators here, and we can see that female has a greater probability.

## Title 21 - Naive Bayes 
When using Bayes rule, one of our problems is calculating the probability of a given data point, given a class. &gt;&gt; You mean p of d given c sub j like we were talking about before. &gt;&gt; And so far in our examples we used 1 d data. &gt;&gt; What happens when we use multiple features? Then things could get pretty complicated. But if we assume that each feature is independent of the others, then it becomes easy again. That independence assumption leads to the naive Bayes technique which is surprisingly powerful. &gt;&gt; Can we do an example? &gt;&gt; Sure. Let's assume we want to determine whether a person named Drew is male or female. And we know other features about the person. &gt;&gt; Like whether they are over 170 centimeters tall, or what their eye color is, or their hair length. &gt;&gt; And we have a database of such data, from which to trade our male versus female classifier. &gt;&gt; Okay. Hold it. We already know how to do this problem. We've seen it before. What do you mean? &gt;&gt; Well I got inspired when you said something about assuming all the features are independent of each other. If that is the case, we can represent the problem as a base net. &gt;&gt; Go on. &gt;&gt; The features are really conditionally independent, based on that class. We are assuming that each class has different distributions for the features. Thus, we can model the net, such that the class c sub j is at the top, with arrows going to each of the features. &gt;&gt; Because the class is the cause and the feature distributions are the effects. &gt;&gt; Right. Once the class is established, each node in the net is independent. In other words, conditional independence based on the class. &gt;&gt; Yep, and we can represent any naive Bayes classifier in this sort of general framework with the class pointing to the features. In addition, our calculations become easier. &gt;&gt; How so?

## Title 22 - Maximum Likelihood 
the probability of class CJ generating the data d simply the product of the the probabilities of the class generating each feature. In other words, class c sub j generating feature d sub one times the probability c sub j featured d sub two times all the rest of the probabilities until we get to the last feature, d sub n. &gt;&gt; Yep the more efficient way to write that equation is that p of d given Cj is equal to the product of P of d given i of cj for i from one to n. Assuming that all classes are equally likely the one that maximizes this equation is the one to which we assign the data. &gt;&gt; Hold it. You slipped in that all classes are equally likely. &gt;&gt; True. Technically this way of determining to which class the data belongs is called maximum likelihood. &gt;&gt; But why would we assume all classes are equally likely? &gt;&gt; Perhaps we don't trust our priors, given a new situation, or we have too little training data. Or our priors really are approximately equal. &gt;&gt; Or we're too lazy to do maximum a posteriori learning. &gt;&gt; What? &gt;&gt; It's the version of Bayes learning that weighs the hypothesis by the priors. Maximum likelihood can be thought of as a special case of maximum a posteriori. &gt;&gt; Okay. But let's continue on with our example. We're trying to give the intuition here. &gt;&gt; Yeah, good idea. Like I said, I like to get the intuition first, and then stare at the book until I get the math. So for our specific case here, we get an equation that looks like this. &gt;&gt; Correct, and the naive bayse independent assumption gives us quite a few nice benefits. &gt;&gt; Like what? &gt;&gt; First, it is space efficient. We only have to store the probability tables for each feature, not all combinations of features. And classifying with Naive Bayes is pretty fast. Just a series of table lookups and multiplication. &gt;&gt; All good things. &gt;&gt; Also, we could always switch back to the full Bayes network way of doing inference if we find that the features really are not independent and we have to start including arts with different feature nodes. &gt;&gt; So basically you're saying the same Bayes net inference methods will work for Naive Bayes and will run pretty fast. But we always have more representation power if we need it. &gt;&gt; Yep, and what I find useful but naive Bayes is that it is not sensitive to irrelevant features. &gt;&gt; Okay, I was following you up until now. But we'll have to spend some more time on this one. What do you mean? &gt;&gt; Suppose we were trying to classify a person's sex based on the features we described before, like eye color. &gt;&gt; But eye color and gender are not linked. You know that and I know that, but the technique doesn't. Suppose we just blindly use all the features we have. What happens? &gt;&gt; Well, let's look at the case of a person named Jessica. And we are going to use features like eye color and whether he or she wears a dress to determine gender. I feel like we're going to get in trouble for this. &gt;&gt; Anyway I have database of 20,000 examples. &gt;&gt; So we're data driven. &gt;&gt; Yep. In that database, we have 9,000 females who have brown eyes and 9,001 males. &gt;&gt; So this irrelevant feature basically cancels out. &gt;&gt; Yep, and the features that are more discriminating, like whether Jessica is known to wear dresses have a strong effect. &gt;&gt; Precisely. That's pretty neat. &gt;&gt; It is, and something I didn't fully appreciate until now. I mean, Bayes really is pretty spiffy. &gt;&gt; Let's do another example of naive Bayes as a quiz.

## Title 23 - Naive Bayes Quiz 
Here's the Bayes network representing a spam detector where the class is whether a message is spam or not, and the features of a message being spam are certain keywords in the message. Here we have Piazza, Bank, and Diplomat. Piazza might be the least likely of the use to be seen in a spam message, while Bank and Diplomat are more likely, but still could appear in regular email as well. Find the probability that a particular email is spam given that it has the word bank, but not piazza and not diplomat.

## Title 24 - Naive Bayes Quiz Solution 
To find the answer, we're first going to apply Bayes rule. So the probability of span given not Piazza, bank, and not diplomat, turns into the probability of not piazza, bank, and not diplomat given span, times the probability of spam all over the probability of not piazza, bank, and not diplomat. We can break the numerator up further into the probability of not piazza, given spam times the probability of bank, given spam times the probability of not diplomat given spam, all times the probability of spam. And then this will all be divided by the probability of not piazza times the probability of bank times the probability of not diplomat. Now we can go ahead and substitute in some values from the ones you were given before.

## Title 25 - No Free Lunch 
Okay, we need to keep moving. We have a lot to cover yet. &gt;&gt; But we have two good pattern recognition pattern algorithms already, KNN and naive Bayes. Why do we need anything else? &gt;&gt; Because there is no free lunch. &gt;&gt; What? &gt;&gt; The no free lunch theorem. It basically states that no one particular algorithm is optimal for all problems. Specifically, Wolpert and Macready state for any algorithm, any elevated performance over one class of problems is offset by performance over another class &gt;&gt; So you mean that naive Bayes won't work for some problems? &gt;&gt; It might not work as well as another algorithm. &gt;&gt; Can we show that? &gt;&gt; Sure.

## Title 26 - Naive Bayes vs kNN 
Let's look at our original problem. &gt;&gt; The shark bites again? &gt;&gt; Well, you asked. We can think about any of our classifiers as making a decision boundary. &gt;&gt; Even kNN? &gt;&gt; Yep, let's draw the decision boundary for one nearest neighbor. &gt;&gt; Wow, that's pretty complex. &gt;&gt; Yeah, it doesn't look anything we saw with maximum likelihood. But let's try it anyway. What do you think the decision boundary would look like if we assume both classes were equally probable, we mulled each class with a Gaussian? &gt;&gt; Ouch, that's pretty hard to visualize. &gt;&gt; Yeah, but give it a try. &gt;&gt; How about this? &gt;&gt; That will do. So the blue elipse is the one standard deviation for iso line for the plus class. &gt;&gt; And the red ellipse is the one standard deviation contour for the O class. &gt;&gt; What do you think the decision boundary will look like? &gt;&gt; Why do you give me the hard problems? &gt;&gt; because I'm lazy. &gt;&gt; Fine. Maybe a parabola that looks like this. &gt;&gt; Actually the decision boundary is going to work better than I thought it would at first. &gt;&gt; Yeah, but look at all the border cases it won't get right. It's probably not as good as the kNN algorithm. &gt;&gt; It depends. &gt;&gt; On what? &gt;&gt; On how noisy the data is. The underlying process that generates the data really is Gaussian, &gt;&gt; Then this representation might be better in the long run, as we classify more and more data. &gt;&gt; But if the true decision boundary is more complex. &gt;&gt; Then all the cavities and complexities of the kNN boundary are going to be needed. Hey, can't we use multiple Gaussians to better this data? &gt;&gt; Sure.

## Title 27 - Using a Mixture of Gaussians 
How about we use two Gaussian for each class? &gt;&gt; Wow that creates a much more complex decision boundary. So basically we can get any arbitrary decision boundary by adding more Gaussian? &gt;&gt; Yep it's a trick called a mixture of Gaussian which we'll come back to later. &gt;&gt; How do we keep from over fitting? &gt;&gt; What do you mean? &gt;&gt; Well in limit I would make each data point it's own Gaussian, and it would basically be the same as KNM &gt;&gt; hip and in-between the two extremes we can use fewer Gaussians which would cause the cision boundary to be smooth. But still give a good continuous approximation of the shape and the density of each class. This trick is called kernel density estimation where I have learned it as parsum window density estimation. In literature, the Gaussian kernel is often referred to as a radio basis function. &gt;&gt; So, I noticed you didn't answer my question about overfitting. &gt;&gt; Well, we can use cross validation to try to pick the number of Gaussians that give the best results. &gt;&gt; Just like we use it to figure out what K should be for KNN. &gt;&gt; Yep. &gt;&gt; And we can even use it to compare different methods, right &gt;&gt; Yep we can, but again we have a danger of accidentally overtuning both the method and its parameters to a particular training set. &gt;&gt; Which is why that final independent test set is very important. &gt;&gt; That's correct.

## Title 28 - Generalization 
We've shown when K and N might provide better decision boundaries, than naive base, and Gaussian mixture models. &gt;&gt; And warn of the dangers of over fitting. &gt;&gt; But when does having a smoother boundary help us? &gt;&gt; Well let's look at the case where we don't have enough training data. Where would we put the decision boundary, if we're using one nearest neighbors for this problem? &gt;&gt; Here. &gt;&gt; And if we were using maximum likelihood, modeling the classes with Gaussians? &gt;&gt; Probably around here. &gt;&gt; But suppose we get one more piece of training data that looks like this? &gt;&gt; Okay, now that would cause the one year's neighbor boundary to split here, here and here. But the data is really starting to look like it's being generated by Gaussian processes. So I bet that this boundary is going to be better for us long term. &gt;&gt; Why not use three nearest neighbor? &gt;&gt; We could, that would lead to one threshold for the boundary again, but we're still sensitive to individual trading points on the boundary possibly messing us up. &gt;&gt; Actually let's use an extreme example to show the problems of over generalization with K nearest neighbor. &gt;&gt; Okay. &gt;&gt; What happens as we make K big? &gt;&gt; Well, we already said it smooths the decision boundary. You mean like using all the data points in the training set? &gt;&gt; Yep. &gt;&gt; In this case, everything would be classified as the left red class because it has six examples and the blue class only has five. I guess that's why there's no free lunch. We need to find a balance between the method which classifies data with the highest accuracy, doesn't over fit and generalizes well for trading data to our unseen data without losing its discrimination power. &gt;&gt; And that is why there are so many different methods in machine learning. &gt;&gt; How do we choose one?

## Title 29 - Visualization 
One of the first things to do is visualize the data to get a sense of it. If most of the classes from balls of data without many concavities, then modeling it with Gaussians will probably work well. If there are situations where classes interpenetrate, but still have distinct boundaries, then k-nearest neighbors or one of the kernel methods will probably work. &gt;&gt; Sometimes the data is so highly dimensional that it's hard to visualize. For those situations, we can also use methods like decision trees and boosting to help you understand which features are most important. &gt;&gt; We should probably talk about those techniques next.

## Title 30 - Decision Trees with Discrete Information 
Decision trees are another machine learning technique that is easy to understand and can often reveal features about a dataset that might not be immediately obvious. &gt;&gt; Why don't we get started with an example? &gt;&gt; Okay, suppose we like to play tennis. &gt;&gt; I'm more of a table tennis guy, but okay, I enjoy hitting the ball around on the court. &gt;&gt; Since we live in Atlanta we don't like to play when it's sunny and humid because we'll get heat stroke. &gt;&gt; But since it's often rainy in Atlanta, we'll still play as long as it isn't too windy. &gt;&gt; Because then you could just get too cold. The right sort of day is an overcast one. &gt;&gt; We can capture all that in a decision tree. &gt;&gt; I see. First, we look to see what the forecast is. If it is going to be overcast, we'll reserve a tennis court. But if it's going to be sunny, then we have to see if it's also going to be humid, in which case we won't play tennis. &gt;&gt; But if the humidity is going to be normal, we'll reserve the court. &gt;&gt; And if it is predicted to be rainy, then you have to look at another feature, the windiness, to see if we are going to reserve the court or not. Decision trees look pretty simple to use. &gt;&gt; They are, and fast, too. Here's a table of a few days' worth of data. Across the top is humidity, wind, outlook and whether we play or not. &gt;&gt; So we look outlook first each time? &gt;&gt; Yes, because it's at the top of the decision tree. &gt;&gt; For this example, since it's sunny, we're going to look at humidity next to see if we're going to play. But in this example all we need to see is that the outlook is overcast, and we have our decision.

## Title 31 - Decision Tree Quiz 1 
Now let's take a look at the last combination of conditions. What is the decision in this case, will we play tennis?

## Title 32 - Decision Tree Quiz 1 Solution 
The answer is yes. First, we check outlook and we see that it's raining. Then we check wind and we see that it's forecasted to be weak. So our answer is yes, we will play tennis.

## Title 33 - DTs with Continuous Information 
What happens if we have continuous values instead of a discrete values? &gt;&gt; Then we just create a threshold for the attribute. For example, what is the tree to discriminate between these three classes? &gt;&gt; Well that's pretty easy. If x is less than threshold 1, then it's the orange class. Otherwise we need to look at the y value. If the y value is above threshold 2, then it's the red class. Otherwise its the blue class.

## Title 34 - Minimum Description Length 
Let's talk about more complex decision trees. &gt;&gt; Okay. &gt;&gt; Here's a decision tree from the Russell and Norvig book which the authors claim is how they decide if they're going to eat at a restaurant. If there are no customers the restaurant must be bad, so we don't want to eat there. If there are some customers then it must be okay, so we'll have dinner. &gt;&gt; But if it is full then it gets complex. &gt;&gt; Right, if the wait is short then they'll wait for a table. If it is over 60 minutes, then they'll leave. &gt;&gt; But between ten minutes and 60 minutes wait, the algorithm gets more complicated and looks to see if there is things like a bar, if there's an alternative nearby, if there's a reservation, and so forth. &gt;&gt; Here's data from 12 outings that we can learn a decision tree from instead of trusting what Russell and Norveig claim in their algorithm. Okay, but you don't trust Russell and Norveig? &gt;&gt; Well, they are AI researchers after all. &gt;&gt; Hey, I resent that remark. &gt;&gt; Anyway, if we learn the decision tree, we'll get something like this. &gt;&gt; Hold it. That seems a lot simpler than the original. What's the difference if it gets the same results? &gt;&gt; Well maybe our training examples do not cover all the possible situations. &gt;&gt; Suppose they did? &gt;&gt; Well, simpler is better. There's a concept of minimal description length in compression and machine learning. You want the sum of the number of bits needed to describe your compression algorithm. And the bits needed to then express the data you want to transmit, using that algorithm, to be as small as possible. &gt;&gt; Well, that gets into what we're going to discus next, information theory. To create a compact decision tree, we want to ask a question whose answer provides the most information towards the problem. Here's the idea. We want an attribute that will split our training examples into all positive and all negative examples. But there is no one attribute that does that. Otherwise, the problem would be trivial. &gt;&gt; Well, which attribute goes the farthest towards that goal? &gt;&gt; I guess patrons. It conclusively classifies six of the 12 examples. &gt;&gt; Okay. Which attribute is the worst? &gt;&gt; Well, I guess it would have to be type, as it doesn't provide an answer for any of the training examples.

## Title 35 - Entropy 
Before we go on, we should be more formal about what we mean by information. Here's a definition of entropy which is a measure of uncertainty and unpredictability in a random variable. &gt;&gt; We can also use it to quantify the amount of information in the message. &gt;&gt; We're going to use entropy to determine how many bits of information we need to solve this restaurant problem. &gt;&gt; Okay. &gt;&gt; Here we have a binary decision. To stay or not to stay at the restaurant. &gt;&gt; So it goes from one to two in this case. And since our goal is to separate out 6 positive and 6 negative examples for a total of 12 examples, we get, minus 6 over 12 times a log base 2 of 6 over 12 minus the quantity, minus 6 over 12 times log 2 of 6 over 12. Which actually equals 1 bit. &gt;&gt; Right. So we need 1 bit of information to solve this problem. As we go forward, we're going to use a simpler form of the equation for binary cases. B of q is equal to negative the quantity q times log base 2 of q plus 1 minus q times log base 2 of 1 minus q. &gt;&gt; And if we have p positive examples and n negative examples, we will represent it as B of p over p plus n.

## Title 36 - Information Gain 
Okay, so now we need to figure out which attribute we should use first in our decision tree. We will do that using information gain. &gt;&gt; A test on a single attribute A will probably give us only part of the one bit we need. We can measure exactly how much by looking at the entry remaining after the attribute test. &gt;&gt; An attribute A, with d distinct values, divide the training set E into subsets E1 through Ed. Each subset E sub k has p sub k positive examples and n sub k negative examples. So if we go along that branch we will need an additional B of p sub k / p sub k + n sub k bits of information to answer the question. A randomly chosen example from the training set has a kth value for the attribute. But probably p sub k + n sub k / p + n. So the expected entropy remaining after testing attribute A is the sum of k = 1 to d of p sub k + n sub k / p + n times B(p sub k / p sub k + n sub k). &gt;&gt; The information gain from the attribute test on A is the expected reduction in entropy. B (p / p + n)- Remainder(A). &gt;&gt; Now we can figure out which attribute is the most important one to use first. For patrons, the gain of patrons is equal to 1- [2/12, because we have 2 examples where we go because there's no patrons in the restaurant, times B(0/2) + 4/12 B(4/4), which is to represent the case where we have some patrons in the restaurant and we actually stay there, + 6/12 B(2/6)], which represents the situation where it's full, and sometimes we stay and sometimes we leave. That gives us approximately 0.541 bits. The gain for type is equal to 1 again, remember, we're trying to get one bit of information,- 2/12 B(1/2), which represents when the type is French. + 2/12 B(1/2), which represents Italian. 4/12 B(2/4), which is Thai. And finally, the Burger joint, which is 4/12 B(2/4). But because all of these numbers end up canceling each other out, we get 0 bits. &gt;&gt; So now we know that we should use patrons for our first attribute at the top of the tree. &gt;&gt; What do we do next? &gt;&gt; We take the remaining examples that patrons does not decide and iterate on the process until we have no more examples to explain. &gt;&gt; That seems easy enough. &gt;&gt; Here's something interesting. Remember that Type did not provide us much information for the top level? &gt;&gt; Yeah? &gt;&gt; Well, two levels down it will actually provide the most information gain. &gt;&gt; That's cool. Is it possible we will use the same attribute repeatedly in the same decision tree? &gt;&gt; Yes, that does happen sometimes. &gt;&gt; Interesting, let's try another example.

## Title 37 - Decision Tree Quiz 2 
Here's a set of decisions. Given this database, calculate the information gain for each attribute. Outlook, Temp, Humidity, and Wind.

## Title 38 - Decision Tree Quiz 2 Solution 
Let's go through how to find the answer here. Recall these formulas from our last lecture. To find the information gain, we have the total entropy for the situation. Minus the remainder after that attribute is taken care of. So first we need to calculate total entropy of our situation. We have nine positive examples out of 14 total examples in this situation. So you can go through the formula for B. You have negative 9 over 14, which is .643* log base 2 of that number, minus 1 minus .643* log base 2 of that number, which is .940 bits. Next, we need to calculate the remainder for each of our situations. Let's start with Outlook. We have three values for Outlook, sunny, overcast, and rainy. We have five examples of sunny, out of our 14 examples in our database. Two of those resulted in a positive decision, so we need the entropy of two out of five. Next we had four examples of overcast and all four of those were positive. Finally for rainy we had five examples and three of those were positive. Going through the calculation gives us 0.246. We also had three values for temperature. Hot, mild, and cool. Again we follow the same procedure to get our value of .028. Humidity and wind only have two values each. You can have a high humidity or a normal humidity. And for wind, we can have a weak wind or a strong wind. Our gain for humidity is 0.151 and our gain for wind is 0.047.

## Title 39 - Random Forests 
Have you heard of random forests? &gt;&gt; You mean forests with strange names like Dark Entry Forest or Crooked Forest? &gt;&gt; No, we're talking machine learning here, remember? &gt;&gt; You mean the ensemble learning technique where you train several decision trees and have them vote on the answer. &gt;&gt; Right. &gt;&gt; Random forests often work quite well for machine learning tasks, but how do they work? It is a bootstrap aggregation technique. &gt;&gt; I prefer the term bagging, especially since the name is so apt for the algorithm. &gt;&gt; Sure. It's a bagging technique where you use random sampling of the training data, a random selection from the attributes in that data, to create multiple decision trees. &gt;&gt; That's it. &gt;&gt; Well, when given unknown data, those decision trees vote on the result. The idea of having multiple techniques voting on the outcome is often called a mixture of experts, but here is just a mixture of decision trees. &gt;&gt; Yep. &gt;&gt; So why does this technique do better than a single decision tree? &gt;&gt; The random sampling seems to help avoid over-fitting, which is often a problem with a single decision tree. &gt;&gt; Can't see the forest for the tree? &gt;&gt; Sure, let's go with that.

## Title 40 - Boosting 
One nice thing about decision trees is that they can tell you which features are the most useful automatically. &gt;&gt; How so? &gt;&gt; In my research on activity recognition, sometimes I don't know what data to collect. Is a tennis stroke most easily detected by an accelerometer on the wrist or a gyroscope? Does a sensor on the waist or ankle help? We can use all the sensors we can imagine and derive all the features we want and then let the decision tree learning process help us determine which features we should investigate more. It can help us with the feature selection problem in machine learning. &gt;&gt; Doesn't that lead to large trees that are hard to read? Most decision tree tools, like the one in weka or rapidminer, allow you to specify he number of decision tree leaves it is allowed to use. By setting it to a small number, we get trees tha tare easy for us to examine. &gt;&gt; And by doing lots of trees or by doing a random forest we can see how stable those features are. &gt;&gt; Yep. And we can even begin to quantify the percent contribution each test makes in the tree and determine which features are most complementary. &gt;&gt; So you can use the minimum number of features to get the accuracy you need? Why does that matter? &gt;&gt; Sometimes features take a lot of processing power to compute or the sensors that generate them require too much battery power. When using the sitting trees, you can optimize your classifier based on speed, power, space and many other criteria. Sometimes though, the only way to get high accuracy is to use lots of different features. &gt;&gt; That reminds me of boosting. &gt;&gt; What? Talking about lots of different features? &gt;&gt; Yeah. The idea behind boosting is that you can combine many weak classifiers to form an ensemble that can do the classifiering task. &gt;&gt; Want to explain that concept? &gt;&gt; Sure. Imagine we have ten items in our training set. Half of them are plusses and half are minuses. &gt;&gt; Okay. &gt;&gt; Now we are going to limit ourself to simple horizontal or vertical decision boundaries. &gt;&gt; Any reason not to include diagonals, or even the quadratic decision boundaries we saw with the Gaussians? &gt;&gt; Not really, but it's just easier to visualize to start with. &gt;&gt; Okay, well what's next? &gt;&gt; Let's start by picking a weak classifier that does the best job it can given the limitations. &gt;&gt; Well there are a couple I can see that would carve off two examples cleanly. Both here and here. &gt;&gt; Okay, pick one. &gt;&gt; Okay, I'll pick this one. &gt;&gt; That classifies all of the minuses correctly but only gets two of the plusses. So that gives us an error of 30 percent. &gt;&gt; Right. And we're going to plug that error into this equation to get this alpha value. &gt;&gt; And what does the alpha value do for you? &gt;&gt; Two things. It's going to give us a voting weight for this weak classifier. But don't worry about that for now. &gt;&gt; Okay. &gt;&gt; And it also gives us weights by which we increase the train examples we got wrong and decrease important examples we got right for the next generation. If the example is classified wrong we multiply it by e to the alpha sub t. If the example is classified correctly, we multiply it by e to the negative sub alpha t. &gt;&gt; In this case, the three pluses that we got wrong are now magnified for their next level, and everything else is smaller than when we started. &gt;&gt; Given this weighting, we now choose another weak classifier that best divides the examples. &gt;&gt; Okay, it looks like we'll use another vertical boundary. &gt;&gt; And we'll get a smaller error this time. &gt;&gt; But a bigger alpha value. &gt;&gt; Which means we'll weight the three minuses we got wrong even more strongly. &gt;&gt; Now what? &gt;&gt; We choose another weak classifier. &gt;&gt; Okay this time a horizontal boundary gives us the best result. &gt;&gt; And our error value is smaller and our alpha bigger. &gt;&gt; And I assume we continue iterating. &gt;&gt; Actually we are done. &gt;&gt; Well we can now classify all the examples in the training set with the weighted [INAUDIBLE] of all the weak classifiers. Where do the weights come from? &gt;&gt; They are the alphas we calculated along the way. &gt;&gt; I don't think I believe you. &gt;&gt; Try it out. &gt;&gt; Okay. Let me try this top right negative example in the training set. &gt;&gt; Well, the first week classifier labels it a minus, so that gives us 0.42 times negative 1 so far. And a second weak classifier says it's a minus. So we have -0.65 + -0.42 which equals -1.07. But the last classifier gets it wrong and adds 0.92 to the score. But the overall score is now -0.15, which is negative, so the ensemble labels it a minus, which is correct. &gt;&gt; So we just keep monitoring the error during boosting and stop when it converges? &gt;&gt; Exactly. &gt;&gt; So I guess, just like decision trees, boosting can help us figure out which features are most interesting for recognition. &gt;&gt; Correct. &gt;&gt; That's cool but I'm not sure I trust this yet. Let's verify some more examples.

## Title 41 - Boosting Quiz 
Here's a new training set of positive and negative examples. Calculate the alpha value corresponding to each boundary choice here. Then, pick the boundary that best classifies these samples on its own.

## Title 42 - Boosting Quiz Solution 
Here are the alpha values for each boundary. Recall that this is the formula for alpha sub T, where epsilon sub T is the number of misclassified examples. If we were only choosing one boundary, we would choose this one, since it only misclassifies these three negative examples. This gives it the highest alpha value of 0.88.

## Title 43 - Neural Nets 
One of the oldest techniques in machine learning is neural nets. It is becoming popular again now. &gt;&gt; Why do we call it neural nets? &gt;&gt; Because it was inspired by the neurons in the brain. The brain has about 10 to 11 neurons of over 20 types. There are lots of connections between neurons, and these connections are called synapses. There are approximately 10 to the 14th synapses in the brain and their cycle time is between one to ten milliseconds. The signals are noisy spike trains of electrical potential. &gt;&gt; Okay, so what does that have to do with us? &gt;&gt; Well, in the 1950s, two researchers by the name of McCulloch and Pitts decided they would try to model neurons using inputs of biased weight. A non-linear function that represent the neuron's cell body and outputs. &gt;&gt; What types of non-linear functions are we talking about? &gt;&gt; One was a simple step function, while the other was a probit, which is basically the Gaussian integrated. &gt;&gt; Which is better? &gt;&gt; Like everything else in machine learning, it depends on your problem, but a lot of people prefer the more smooth nature of the probit activation function. &gt;&gt; But it's pretty easy to just do the basics of computing using the step function. Just by changing the input weights and the bias weight I can do an AND and a NOT gate. That's all I need for logic and to do computing in general. And that was one the reasons why early researchers were so excited. They had shown that their simple model could do general computing. At least, in theory. Now, let's pause for quiz.

## Title 44 - Neural Nets Quiz 
Fill in this truth table to create neural net for the nor function, given that i1 and i2 are the inputs and a is the activation output. What are the bias weights and the input weights in this case? And what type of activation function would you use here?

## Title 45 - Neural Nets Quiz Solution 
Here's the answer. We'll use a step function here, because this is a binary function. Here is one option for the weights. You can see that when both our inputs are 0, we'll have a positive value for our bias. And as long as we set our threshold appropriately, we'll have a 1 at the output. For all other combinations, we'll get a negative value, which we can output as a 0. There are other possible combinations of these weights, but as long as they follow the truth table and also weight the inputs equally, they can still work.

## Title 46 - Multilayer Nets 
So I guess the next step was to combine these neurons in a massive way like in the brain. &gt;&gt; Well that was the hope, but the electronics and the computers at the time were much slower than now and large nets were difficult to simulate. However, the researchers knew they were on to something. &gt;&gt; Let's take a look at a simple feed forward network. &gt;&gt; Hold it. Feed forward network? &gt;&gt; That means it has no internal state. Feed forward networks basically implement functions on their inputs. &gt;&gt; But that implies that there is a type of network that does have internal state. &gt;&gt; Those are recurrent networks. They have directed cycles with delays, like flip flops in electronics. &gt;&gt; Okay. I could see how we can do that. &gt;&gt; Let's go back to this example. &gt;&gt; So this feed forward network has two inputs, labeled as these squares here. &gt;&gt; Right. &gt;&gt; Now what? &gt;&gt; Well, let's go in reverse. The output of this network a5 is basically the output of its activation function g, with its input being the weight W35 x a3, which is the output node 3. Plus W45 times the output of node 4, which is a4. &gt;&gt; But we can break that down even more because we know the output of the middle nodes is simply the activation function run on the output of the previous layer times the weights. &gt;&gt; Which means we can write the output of the speed forward network like this. &gt;&gt; Hey, I have a question. &gt;&gt; What's that? &gt;&gt; What if the output function is linear? &gt;&gt; Then the entire network can be reduced to a linear combination of the weights, but then you lose the power of the network. &gt;&gt; I see. The nonlinearity of the activation function allows each neuron to make its individual contribution to the decision boundary. &gt;&gt; Yep, and by changing the weight slowly through training, we can iteratively improve the decision boundary.

## Title 47 - Perceptron Learning 
To show learning in neural nets, we're going to start with a single layer perceptron. &gt;&gt; That looks like two layers to me. &gt;&gt; We generally don't count the input layer. And all the units operate separately as there are no shared weights. &gt;&gt; Let's look at this middle unit. Assume we are using the sigmoid as the activation function. Suppose the white squares have zero output so that only the two black inputs are contributing. The output of this middle unit is going to be a two dimensional function of those inputs. &gt;&gt; We can visualize it using this graph. Either one or the other input has to be pretty high, or they both have to be firing somewhat for the output of this unit to go high. Hey, I remember a story about one of the first uses of perceptrons. &gt;&gt; What's that? &gt;&gt; The goal was to make a piece of hardware to do optical character recognition on simple letters by using a grid of squares. Each square had a photo cell above it so it could tell if the square was white or black. On the other side of the box was a set of lights that correspond to the letters learned. Like H, I, and T. &gt;&gt; They made a real piece of hardware to do neural nets? &gt;&gt; Yep, it was before there were interactive computers, so they implemented everything in analog hardware. &gt;&gt; That's pretty cool. &gt;&gt; Yeah, I find thinking about the problem in this context really helps make it concrete for me. &gt;&gt; Okay we'll use it for demonstrating how to learn the ways for a perceptron. &gt;&gt; Sounds great. &gt;&gt; First we have to figure out an error metric. We are going to use squared error. Which, in the case where our input is x, is simply the square of the value of the output we want, y. Minus the result of the neural net, hW, where it is applied to input x. &gt;&gt; I get it. So let's look at the OCR hardware system I was talking about. Let's input vector x as a three by three grid of pixels and we just want to recognize a few letters like H, I, and T. Each of the units represent a letter. Like the first one could be an H, the middle one could be I, and the last one could be T. &gt;&gt; In that case, we'd show the I on the input side and we'd see the output units in the display. It would start out random, but we would want it to have the middle unit activated and the other two with no activation. &gt;&gt; So we'd subtract the current perceptron's output, which we'll think of as a vector, the desired output vector of 010, and square it. &gt;&gt; Yep, and now we are ready to use gradient descent to optimize a result. In this graph, the middle unit has nine weights to optimize. We're going to calculate the direction of the gradient to minimize the error for the first weight, W0. &gt;&gt; I get it. Since this space is nine-dimensional, we're going to calculate the gradient for each component, W0 to W8 separately, and then adjust the weights to minimize the error. &gt;&gt; Not quite. Remember that we have multiple training examples for which we need to optimize the weights. If we optimize for one example and then the next- &gt;&gt; We might end up thrashing between weight values. Okay, that's a problem. I can see that we might present the letter I to the perceptron, have it optimize the weights so that the I middle unit is maximally activated for this situation. But then, when we show the perceptron T, the I middle unit is lit quite a bit when we want it to be off. Similarly, train T will have the same problem. We need a couple of the weights to really inhibit I, based on the difference between it and T. But it's not obviously without having knowledge of both I and T at the same time. What's the solution? &gt;&gt; We're going to have a learning weight, alpha, that we're going to use to change the weight slowly. That way each example gets a chance to contribute to the weights over time. And we iterate, showing the perceptron the examples in the training set, over and over again. Slowly adjusting the weights, and converging on the set that provides least error. &gt;&gt; That makes a lot of sense. It is similar to the iterative improvement algorithms we talked about back with some related genetic algorithms and the stochastic beam search. &gt;&gt; Exactly. &gt;&gt; These models seem pretty simple. What sort of problems can they solve?

## Title 48 - Expressiveness of Perceptron 
Well a single layer perceptron can only do linear decision boundaries. &gt;&gt; That seems limited but still useful. With a linear boundary we can do and and or. &gt;&gt; But not xor and that's a problem. However, a perceptron can learn a function like majority quickly, whereas a decision tree has a lot of problems. Here's the performance of a decision tree and a perceptron being trained on the majority function with 11 inputs. &gt;&gt; I guess that makes sense. For decision tree, it would need to make a full tree with 11 levels in order to do a good job. It would really need to see 2 to the 11th or 2,048 unique examples to really get it. &gt;&gt; Yet the preceptron does pretty horribly with the restaurant example we did earlier. &gt;&gt; The decision tree does much better with a lot less examples. I guess there is no free lunch. &gt;&gt; True. &gt;&gt; Can we make perceptrons better? &gt;&gt; Sure.

## Title 49 - Multilayer Perceptrons 
The trick is to allow hidden units in your neural net, like in this diagram. &gt;&gt; How does that help? &gt;&gt; Well, remember when we showed that cliff like function with the perceptron? &gt;&gt; Yeah. &gt;&gt; What if we combine two of those in the next layer of the neural net? &gt;&gt; Then we could create a ridge like this, and be able to model any continuous function. &gt;&gt; And if we went up to three layers of units, we could combine those ridges to create bumps like this. That would allow us to model any function. &gt;&gt; Wow! It's almost like our Gaussian mixture models before in kernels. &gt;&gt; Yep, these ideas are all interrelated in machine learning. Many of these techniques can work for the same problem but each technique has its own advantages and disadvantages. &gt;&gt; Well, you've got me interest in neural nets, how do we train one within layers?

## Title 50 - Back-Propagation 
With a technique called back-propagation. It's actually pretty simple. &gt;&gt; Well, I see we start out the same way as with the perceptron for the output layer. &gt;&gt; Yep, we calculated our area looking at what the output should be and what it is for our training example. &gt;&gt; To update the connections between the input units and the hidden units, we need to define a quantity analogous to the error term for output nodes. In other words, we need to propagate the error back through the hidden nodes towards the input nodes. &gt;&gt; The idea is that a hidden node, J, is responsible for some fraction of the error, delta sub i, in each of the output nodes to which it connects. Thus, the delta sub i values are divided according to the strength of the connection between the hidden node and the output node. And are propagated back to provide the delta sub j values for the hidden layer. The propagation rule for the delta values is delta sub j is equal to g prime of in sub j, times the sum over all i of W sub j i, times delta sub i. &gt;&gt; Okay, and now that we have the back-propagated error, we can create the weight update rule for the weights between the inputs and the hidden layer, which is essentially the same as the update rule for the output layer. In other words, W sub k j is equal to W sub k j, plus alpha, times a sub k, times delta sub j. &gt;&gt; So basically the algorithm is computing the delta values for the output units using the observed error. Then, starting with the output layer, we propagate the delta values back to the previous layer and update the weights between the two layers. Then iterate until the earliest hidden layer is reached. &gt;&gt; In neural nets, this process of updating the weights and then summing the gradient updates for all the training examples is called an epoch. And machine learning researchers often can tell how hard a problem is by how long it takes for all the weights to converge to get a minimum error on the training set. &gt;&gt; Remember the restaurant example which the decision tree could handle but the perceptron couldn't? Here is a training curve showing their error converging on a training set of 100 examples, for a neural net with four hidden units that can find an exact fit. &gt;&gt; And here is a graph showing performance on the restaurant problem using a test set and different-sized training sets. Notice the decision tree gets better results faster, but the multi-layer perceptron eventually gets there.

## Title 51 - Deep Learning 
It seems like neural nets can eventually get good performance on hard data sets. &gt;&gt; Yep and there has been a resurgence of interest in neural nets with multiple layers with what is called deep learning. Deep learning techniques use hierarchical structures to solve complex problems. &gt;&gt; So why not use neural nets on everything? &gt;&gt; Well neural nets can require a lot of computation and examples to train. But, one of the real problems is making the results human understandable. When a decision tree classifies a piece of data in an odd way the user can at least in theory follow the tree and gain an understanding of how the system made it's decision. With neural nets it gets more complex to determine what the system is doing than the limits of it's capability. Yet neural nets can discover new features in the data that can improve performance. &gt;&gt; And we have seen recent improvements in speech and image recognition this way. With something visual like hand writing recognition we can even visualize the outputs of different layers of the the network and gain intuition of how well it is doing. It might still be hard to use the system to gain a better understanding of the problem domain. But for many problems we care more about performance. And when I listen to talks given by NeuroNet researchers I think we are close to making systems that can pull out meaningful structure at multiple levels. With the amount of data and processing power available, these systems are requiring less supervision while make significant advances.

## Title 52 - Unsupervised Learning 
Speaking about supervision, we should talk about unsupervised learning, too. &gt;&gt; This is one of my favorite topics. &gt;&gt; Unsupervised learning means that the algorithm is given a set of data without labels, and it attempts to determine what classes are in the data and what data belongs to which class. &gt;&gt; These algorithms are especially useful when we have large databases that are hard to label, and we need a first pass at determining if there's any structure to the data. I'm currently working with marine mammal researchers to use unsupervised algorithms to hunt for structure in dolphin vocalizations. We have already used these algorithms to uncover structure in sign language, speech, exercise data, and optical character recognition, where we know the classes. But if we can discover some new information about animals from their vocalizations, we will have shown that our algorithms are useful. &gt;&gt; Why don't we introduce unsupervised learning using K-means. &gt;&gt; That's a great idea.

## Title 53 - k-Means and EM 
With K-Means we specify the number of classes we expect to find in a dataset. Here we expect to find two. We will start by putting the two means randomly in the data. &gt;&gt; This method is sounding like iterative improvement already. &gt;&gt; Yep, that is where we are going. We'll use blue and red for convenience for denoting our classes. We will take every point in the database, and assign it to the nearest mean. This step is called expectation. Then we'll recalculate the means based on the assignment of the points to each of the clusters. This step is called maximization. &gt;&gt; That was quick. The means are already converging to the clusters in the data. &gt;&gt; Yep, and now we'll do another expectation step and classify every data point as red or blue, depending on how close it is to the respective mean. &gt;&gt; So we are basically getting a preliminary decision boundary. &gt;&gt; In a sense. And we'll reestimate the means again by averaging the coordinates of all the data points in their clusters. &gt;&gt; And then use those new means to create a new decision boundary and reclassify all the data. &gt;&gt; And estimate the means again. &gt;&gt; And then assign points to each cluster again. &gt;&gt; Until the means and the assigns to the points to the clusters don't change. &gt;&gt; This graph shows the sum of the squares of distances of each data point to its assigned mean. The blue circles show the maximization steps and the red circles show the expectation steps. &gt;&gt; For such a clean example as this one, we quickly converge on the correct two clusters. &gt;&gt; But, what if we get stuck and it does not converge? &gt;&gt; Well, we can try random restart like we did with hill climbing, and see if it converges better with different initial conditions. In fact, random restart is not a bad policy in general to see how stable the result is. &gt;&gt; I suspect that visualizing the data helps determine whether these unsupervised techniques will work well. &gt;&gt; Yep, it does. For good clustering, we want to see data with high intercluster variance, but low intracluster variance. &gt;&gt; In other words, we want to see well separated blobs. &gt;&gt; You got it. And in my research on human activity recognition I found that many activities had this property. Human activity tends to separate well into clusters in both time and space. In the past, my students and I have used month long recordings of GPS positions and unsupervised algorithms to discover locations of significance to a user, like home, work, and grocery store. We could then use this data to predict the user's movement between these locations. This model allows a computerized assistance to provide timely reminders of what to take to work, before even getting in the car.

## Title 54 - EM and Mixture of Gausssians 
It occurs to me that we can use the same trick, to fit a mixture of Gaussians to the data. &gt;&gt; Yep, we talked about using mixtures of Gaussians before, but we did not say how to fit them. We can use the same expectation mechanization approach to find each Gaussian's mean and variance. But before we start doing We need to know how many Gaussians we want to use. Here we'll pick two and start with the identity as the covariance matrix. &gt;&gt; Meaning that they are circles. &gt;&gt; Right. &gt;&gt; Just like with K means, we assign each point of data to the nearest cluster. &gt;&gt; We'll then reestimate the means and variances of the Gaussians. And we'll continue iterating. &gt;&gt; Effectively adjusting the position and the shape of the clusters. &gt;&gt; Until it eventually converges. Note that because we had more parameters to estimate, it took 20 Cycles to converge. Where as it took about four before with K Means. Increasing the number of dimensions to estimate almost always requires more time and more data to complete. &gt;&gt; Expectation maximization is a powerful idea and we see variance of it repeatedly in machine learning, perception, and robotics. Please take some time to go through the readings and understand the details of this technique. &gt;&gt; The next section we'll use For techniques that will allow us to recognize speech, handwriting, sign language, and many other time series.
