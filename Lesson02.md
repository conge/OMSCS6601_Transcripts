# Lesson 2

## Title 1 - Introducing Peter Norvig 
Today we have the privilege of meeting Peter Norvig, whose online course with Sebastian Thrun on AI was one of the first big MOOCs and led to Udacity and a lot of entrepreneurial interest in teaching online classes. He's the director of research at Google and previously directed Google's core search algorithms group. He worked as a top NASA computer scientist, taught at Berkeley, and wrote the most popular AI textbook ever, which hopefully you all are enjoying right now. Thanks for coming today Peter. &gt;&gt; Well thanks for having me here Thad. I'm glad to see so many students interested in the course and in the Masters program.

## Title 2 - Intro to Search
Peter is going to teach us about search. While we already did a lot of searching in the game playing section, most search algorithms are not adversarial. Peter, why is search so important in AI? &gt;&gt; Well, to me, AI is all about figuring out what to do when you don't know what to do. Regular programming is writing instructions to make the computer do what you want when you do know what to do. And AI is for when you don't know. And search is one of the key areas where we can figure out what to do by planning a sequence of steps, even when we have no idea what the first step should be until we solve the search problem. &gt;&gt; When watching these lessons, please pay careful attention to the A* algorithm. It is one of the most famous AI algorithms, and a lot of job interviewers in the field will assume you are familiar with its concepts. Let's do a challenge question to give you a preview of what you'll be learning.

## Title 3 - Challenge 1  Tri city Search
The Carter Center in Atlanta is famous for monitoring elections to make sure they are fair. This year, they decided to have their interns practice on local elections. Sally, a Georgia Tech computer scientist turned activist, needs to visit these three polling stations on election day. The night before, she plans to stay at a hotel close to one of the polling stations. After checking the voting machines at the first station, she will visit each of the other two in turn. Sally is on her bicycle, so she wants to minimize her travel distance, and she is willing to go the wrong way down one way streets to do it. Fortunately, Sally took the artificial intelligence course. So she knows how to write a program that tells her where to start and the optimal streets used to get to each polling station. Proud of being a nerd, Sally wrote her program such that it had minimal runtime and memory overhead. How did she do it? Choose the best answer. Remember, these challenge questions are designed to give a preview of the lesson to come. We do not expect you to know how to do the answer yet.

## Title 4 - Challenge 1  Tri city Search Solution
There are many ways to solve what we'll call the tri-cities problem. A straightforward way is do a search, from Buckhead to Scottdale, then from Scottdale to Little Five Points, then from Little Five Points to Buckhead. However, this method would explore many more street intersections than necessary. The trick here is to start the search from each of the three locations simultaneously, using the A* algorithm, which we'll call the tridirectional A* search. So the correct answer is none of the above. We'll revisit this challenge question at the end of the lesson in more detail

## Title 5 - Challenge 2  Rubikӳ Cube
Here's another question, but it's more of a research question. In the 1980s, everyone owned a Rubik's Cube puzzle. The goal was to have all the same colors on the same side. The levels of the cube could be twisted on the vertical or horizontal. How do we design a search algorithm for Rubik's Cube that guarantees the least number of moves required to finish from any starting state? We'll consider each quarter turn to be a move. Choose a search strategy most likely to be part of your algorithm.

## Title 6 - Challenge 2  Rubikӳ Cube Solution
As it turns out, the correct answer is iterative deepening A*. This problem is much harder than it sounds, simply due to the size of the search space. This table shows the number of nodes in the search tree as a function of depth. Clearly the problem becomes intractable quickly. However, in 1997, Richard Korf published an approach using iterative deepening A* search. He had to search to a median depth of 18 for the random configurations he attempted. The question is, what admissible heuristic did Korf invent for his A* search? To find out, follow the link to his paper in the instructor notes. This problem continued to bother computer science researchers. In 2010, Thomas Rokiki and Morley Davidson proved that the maximum number of quarter turns needed to solve the cube is 26, using 29 CPU years of idle computer time at the Ohio Super Computing Center. Games have a long history of inspiring rapid to nasty for AI researchers. As Peter teaches us about search, keep in mind interesting problems like these, and look for clues as to how we might use search techniques to solve them.

## Title 7 - Introduction 
[PROBLEM SOLVING] In this unit we're going to talk about problem solving. The theory and technology of building agents that can plan ahead to solve problems. In particular, we're talking about problem solving where the complexity of the problem comes from the idea that there are many states. As in this problem here. A navigation problem where there are many choices to start with. And the complexity comes from picking the right choice now and picking the right choice at the next intersection and the intersection after that. Streaming together a sequence of actions. This is in contrast to the type of complexity shown in this picture, where the complexity comes from the partial observability that we can't see through the fog where the possible paths are. We can't see the results of our actions and even the actions themselves are not known. This type of complexity will be covered in a later unit. Here's an example of a problem. This is a route-finding problem where we're given a start city, in this case, Arad, and a destination, Bucharest, the capital of Romania, from which this is a corner of the map. And the problem then is to find a route from Arad to Bucharest. The actions that the agent can execute when driving from one city to the next along one of the roads shown on the map. The question is, is there a solution that the agent can come up with given the knowledge shown here to the problem of driving from Arad to Bucharest?

## Title 8 - What Is A Problem  
And the answer is no, there is no solution that the agent can come up with because Bucharest doesn't appear on the map and so the agent doesn't know any actions that can arrive there. So let's give the agent a better chance. Now we've given the agent the full map of Romania. The start is in Arad and the destination, or goal, is in Bucharest and the agent is given the problem of coming up with a sequence of actions that will arrive at the destination. Now is it possible for the agent to solve this problem? And the answer is yes. There are many routes or steps or sequences of actions that will arrive at the destination. Here's one of them. Starting out in Arad taking this step first, then this one. And then this one, then this one, and then this one, to arrive at the destination. So that would count as a solution to the problem. So, sequence of actions, chained together, that are guaranteed to get us to the goal. Now, let's formally define what a problem looks like. A problem can be broken down into a number of components. First, the initial state that the agent starts out with. In our route finding problem, the initial state was the agent being in the city of Arad. Next a function actions that takes a status input and returns a set of possible actions that the agent can execute when the agent is in the state. In some problems, the agent will have the same actions available in all states. And in other problems they'll have different actions dependent on the state. In the route finding problem, the actions are dependent on the state. When we're in one city we can take the routes to the neighboring cities, but we can't go to any other cities. Next, we have. A function called result which takes as input a state. And an action and delivers as its output, a new state. So for example, if the agent is in the city of Arad, and, that would be the state, and takes the action of driving along route E671 towards Timisoara, then the result of applying that action in that state would be the new state where the agent is in the city of Timisoara. Next, we need a function. Called GoalTest which takes a state. And returns a boolean value true or false, telling us if this data is a goal or not. In a route finding problem, the only goal would be being in the destination city, the city of Bucharest, and all the other states would return false for the goal test. And finally, one more thing, which is a path cost function which takes a path, a sequence of state action transitions and returns a number which is the cost of that path. Now for most of the problems we'll deal with we'll make the path cost function be additive so that the cost of the path is just the sum of the individual steps. And so we'll implement this path cost function in terms of a step cost function. The step cost function takes a state, an action, and the resulting state from that action. And returns a number n which is the cost of that action. In the route finding example, the cost might be the number of miles traveled, or maybe the number of minutes it takes to get to that destination.

## Title 9 - Example Route Finding  
Now let's see how the definition of a problem maps onto the root founding domain. First, the initial state we're given. Let's say we start of in Arad. And the goal test, let's say that the state of being in Bucharest is the only state that counts as a goal. And all other states are not goals. Now the set of all the states here is known as the states space. And we navigate the states space by applying actions. The actions are specific to each city. So when we're in Arad, there are three possible actions, to follow this road, this one, or this one. And as we follow them we build paths or sequences of actions. So just being in Arad is the path of length zero. And now we could start exploring the space and add in this path of length one. This path of length one and this path of length one. We can add in another path, here of length two and another path here of length two. Here's another path of length two. Here's a path of length three, another path of length two, and so on. Now at every point, we want to separate the state out into three parts. First, the ends of the paths, the farthest paths that have been explored, we call the frontier. And so the frontier in this case consists of these states and are the furthest out we can explore. And then, to the left of that in this diagram, we have the explored part of the state. And then off to the right we have the unexplored. So let's write down those three components. We have the frontier, we have the unexplored region, and we have. Explored region. One more thing. In this diagram, we've labeled the step cost of each action along the route. So the step cost of going between Neamt and Iasi would be 87, corresponding to a distance of 87 kilometers. And then the path cost is just the sum of the step cost. So the cost of the path of going from Arad to Oradea would be 71 plus 75.

## Title 10 - Tree Search 
[Narrator] Now let's define a function for solving problems. It's called Tree Search because it superimposes a search tree over the state space. Here's how it works: It starts off by initializing the frontier to be the path consisting of only the initial states, and then it goes into a loop in which it first checks to see do we still have anything left in the frontier? If not we fail, there can be no solution. If we do have something, then we make a choice. Tree Search is really a family of functions not a single algorithm which depends on how we make that choice, and we'll see some of the options later. If we go ahead and make a choice of one of the paths on the frontier and remove that path from the frontier, we find the state which is at the end of the path, and if that state's a go then we're done. We found a path to the goal; otherwise, we do what's called expanding that path. We look at all the actions from that state, and we add to the path the actions and the result of that state; so we get a new path that has the old path, the action and the result of that action, and we stick all of those paths back onto the frontier. Now Tree Search represents a whole family of algorithms, and where you get the family resemblance is that they're all looking at the frontier, copying items off and and looking to see if their goal tests, but where you get the difference is right here, in the choice of how you're going to expand the next item on the frontier, which path do we look at first, and we'll go through different sets of algorithms that make different choices for which path to look at first. The first algorithm I want to consider is called Breadth-First Search. Now it could be called shortest-first search because what it does is always choose of the frontier one of the paths that hadn't been considered yet that's the shortest possible. So how does it work? Well we start off with the path of length 0, starting in the start state, and that's the only path in the frontier so it's the shortest one so we pick it, and then we expand it, and we add in all the paths that result from applying all the possible actions. So now we've removed this path from the frontier, but we've added in 3 new paths. This one, this one, and this one. Now we're in a position where we have 3 paths on the frontier, and we have to pick the shortest one. Now in this case all 3 paths have the same length, length 1, so we break the tie at random or using some other technique, and let's suppose that in this case we choose this path from Arad to Sibiu. Now the question I want you to answer is once we remove that from the frontier, what paths are we going to add next? So show me by checking off the cities that ends the paths, which paths are going to be added to the frontier?

## Title 11 - Tree Search Continued 
[Male narrator] The answer is that in Sibiu, the action function gives us 4 actions corresponding to traveling along these 4 roads, so we have to add in paths for each of those actions. One of those paths goes here, the other path continues from Arad and goes out here. The third path continues out here and then the fourth path goes from here--from Arad to Sibiu and then backtracks back to Arad. Now, it may seem silly and redundant to have a path that starts in Arad, goes to Sibiu and returns to Arad. How can that help us get to our destination in Bucharest? But we can see if we're dealing with a tree search, why it's natural to have this type of formulation and why the tree search doesn't even notice that it's backtracked. What the tree search does is superimpose on top of the state space a tree of searches, and the tree looks like this. We start off in state A, and in state A, there were 3 actions, so we gave those paths going to Z, S, and T. And from S, there were 4 actions, so that gave us paths going from O, F, R, and A, and then the tree would continue on from here. We'd take one of the next items and we'd move it and continue on, but notice that we returned to the A state in the state space, but in the tree, it's just another item in the tree. Now, here's another representation of the search space and what's happening is as we start to explore the state, we keep track of the frontier, which is the set of states that are at the end of the paths that we haven't explored yet, and behind that frontier is the set of explored states, and ahead of the frontier is the unexplored states. Now the reason we keep track of the explored states is that when we want to expand and we find a duplicate-- so say when we expand from here, if we pointed back to state T, if we hadn't kept track of that, we would have to add in a new state for T down here. But because we've already seen it and we know that this is actually a regressive step into the already explored state, now, because we kept track of that, we don't need it anymore.

## Title 12 - Graph Search 
Now we see how to modify the Tree Search Function to make it be a Graph Search Function to avoid those repeated paths. What we do, is we start off and initialize a set called the explored set of states that we have already explored. Then, when we consider a new path, we add the new state to the set of already explored states, and then when we are expanding the path and adding in new states to the end of it, we don’t add that in if we have already seen that new state in either the frontier or the explored. Now back to Breadth First Search. Let’s assume we are using the Graph Search so that we have eliminated the duplicate paths. Arad is crossed off the list. The path that goes from Arad to Sibiu and back to Arad is removed, and we are left with these one, two, three, four, five possible paths. Given these 5 paths, show me which ones are candidates to be expanded next by the Breadth First Search Algorithm.

## Title 13 - Breadth First Search 1 
[Male narrator] And the answer is that Breadth - First Search always considers the shortest paths first, and in this case, there's 2 paths of length 1, and 1, the paths from Arad to Zerind and Arad to Timisoara, so those would be the 2 paths that would be considered. Now, let's suppose that the tie is broken in some way and we chose this path from Arad to Zerind. Now, we want to expand that node. We remove it from the frontier and put it in the explored list and now we say, "What paths are we going to add?" So check off the ends of the paths the cities that we're going to add.

## Title 14 - Breadth First Search 2 
[Male narrator] In this case, there's nothing to add because of the 2 neighbors, 1 is in the explored list and 1 is in the frontier, and if we're using graph search, then we won't add either of those.

## Title 15 - Breadth First Search 3 
[Male narrator] So we move on, we look for another shortest path. There's one path left of length 1, so we look at that path, we expand it, add in this path, put that one on the explored list, and now we've got 3 paths of length 2. We choose 1 of them, and let's say we choose this one. Now, my question is show me which states we add to the path and tell me whether we're going to terminate the algorithm at this point because we've reached the goal or whether we're going to continue.

## Title 16 - Breadth First Search 4 
[Male narrator] The answer is that we add 1 more path, the path to Bucharest. We don't add the path going back because it's in the explored list, but we don't terminate it yet. True, we have added a path that ends in Bucharest, but the goal test isn't applied when we add a path to the frontier. Rather, it's applied when we remove that path from the frontier, and we haven't done that yet.

## Title 17 - Breadth First Search 5 
[Male narrator] Now, why doesn't the general tree search or graph search algorithm stop when it adds a goal node to the frontier? The reason is because it might not be the best path to the goal. Now, here we found a path of length 2 and we added a path of length 3 that reached the goal. The general graph search or tree search doesn't know that there might be some other path that we could expand that would have a distance of say, 2-1/2, but there's an optimization that could be made. If we know we're doing Breadth - First Search and we know there's no possibility of a path of length 2-1/2. Then we can change algorithm so that it checks states as soon as they're added to the frontier rather than waiting until they're expanded and in that case, we can write a specific Breadth - First Search routine that terminates early and gives us a result as soon as we add a goal state to the frontier. Breadth - First Search will find this path that ends up in Bucharest, and if we're looking for the shortest path in terms of number of steps, Breadth - First Search is guaranteed to find it, But if we're looking for the shortest path in terms of total cost by adding up the step costs, then it turns out that this path is shorter than the path found by Breadth - First Search. So let's look at how we could find that path.

## Title 18 - Uniform Cost Search 
An algorithm that has traditionally been called uniform-cost search but could be called cheapest-first search, is guaranteed to find the path with the cheapest total cost. Let's see how it works. We start out as before in the start state. And we pop that empty path off. Move it from the frontier to explored, and then add in the paths out of that state. As before, there will be 3 of those paths. And now, which path are we going to pick next in order to expand according to the rules of cheapest first?

## Title 19 - Uniform Cost Search 1 
Cheapest first says that we pick the path with the lowest total cost. And that would be this path. It has a cost of 75 compared to the cost of 118 and 140 for the other paths. So we get here. We take that path off the frontier, put it on the explored list, add in its neighbors. Not going back to Arad, but adding in this new path. Summing up the total cost of that path, 71 + 75 is 146 for this path. And now the question is, which path gets expanded next?

## Title 20 - Uniform Cost Search 2 
Of the 3 paths on the frontier, we have ones with a cost of 146, 140, and 118. And that's the cheapest, so this one gets expanded. We take it off the frontier, move it to explored, add in its successors. In this case it's only 1. And that has a path total of 229. Which path do we expand next? Well, we've got 146, 140, and 229 So 140 is the lowest. Take it off the frontier. Put it on explored. Add in this path for a total cost of 220. And this path for a total cost of 239. And now the question is, which path do we expand next?

## Title 21 - Uniform Cost Search 3 
The answer is this one, 146. Put it on explored. But there's nothing to add because both of its neighbors have already been explored. Which path do we look at next?

## Title 22 - Uniform Cost Search 4 
The answer is this one. Two-twenty is less than 229 or 239. Take it off the frontier. Put it on explored. Add in 2 more paths and sum them up. So, 220 plus 146 is 366. And 220 plus 97 is 317. Okay, and now, notice that we're closing in on Bucharest. We've got 2 neighbors almost there, but neither of them is their turn yet. Instead, the cheapest path is this one over here, so move it to the explored list. Add 70 to the path cost so far, and we get 299. Now the cheapest node is 239 here, so we expand, finally, into Bucharest at a cost of 460. And now the question is are we done? Can we terminate the algorithm?

## Title 23 - Uniform Cost Search 5  
And the answer is no, we're not done yet. We have reached a goal state. We put a path onto the frontier that reaches the goal of Bucharest, but we haven't popped that path off the frontier. And uniform cost search continues to search until we pop it off the frontier. We continue looking to see if there's a better path that also reaches the goal. So let's see. I forgot to say Thagoras is explored, so let's continue. Let's take the cheapest path on the frontier and expand that. The cheapest path is this, 146. We'll expand that, get another path into (Sibiyu). That's a worse path than we had before, so we'll drop it. Then let's see what's next. Looking on the frontier, the cheapest now is here at 299. We'll expand that. We get a path of cost 374. Put that on the frontier. Now let's go again. Now the cheapest path is over here at 317. We'll mark that as explored and add two more paths--one here that's a worse path, so it gets dropped. And one path that also reaches the goal, and that has a total cost of 418. So that just shows it's a good thing we waited, a good thing we didn't stop when we found the first path to the goal, because now this second path found is actually cheaper than the first path found. But we're not going to stop here because we still haven't popped off a path that reaches the goal. So we'll continue. What's next? Now the cheapest path on the frontier is here at 366. We expand that, and we get paths that are worse paths to points we've already seen before. So nothing new goes on the frontier. Next, the cheapest path on the frontier is at 374. Again, expanding that leads nothing useful. Only worse paths than we've seen before. And now finally, the cheapest path on the frontier is this 418 path to Bucharest, so we pop that off, and now we reach the goal, and now we stop. So even though we found the 460 path first, we don't stop there because there might be another path that also reaches the goal that's cheaper. We keep on going until we popped a path off of the frontier that reaches the goal, and that's why uniform cost search is guaranteed to find the cheapest path to the goal.

## Title 24 - Search Comparison 
So, we've looked at 2 search algorithms. One, breadth-first search, in which we always expand first the shallowest paths, the shortest paths. Second, cheapest-first search, in which we always expand first the path with the lowest total cost. And I'm going to take this opportunity to introduce a third algorithm, depth-first search, which is in a way the opposite of breadth-first search. In depth-first search, we always expand first the longest path, the path with the most lengths in it. Now, what I want to ask you to do is for each of these nodes in each of the trees, tell us in what order they're expanded, first, second, third, fourth, fifth and so on by putting a number into the box. And if there are ties, put that number in and resolve the ties in left to right order. Then I want you to ask one more question or answer one more question which is are these searches optimal? That is, are they guaranteed to find the best solution? And for breadth-first search, optimal would mean finding the shortest path. If you think it's guaranteed to find the shortest path, check here. For cheapest first, it would mean finding the path with the lowest total path cost. Check here if you think it's guaranteed to do that. And we'll allow the assumption that all costs have to be positive. And in depth first, cheapest or optimal would mean, again, as in breadth first, finding the shortest possible path in terms of number of lengths. Check here if you think depth first will always find that.

## Title 25 - Search Comparison 1 
Here are the answers. Breadth-first search, as the name implies, expands nodes in this order. One, 2, 3, 4, 5, 6, 7. So, it's going across a stripe at a time, breadth first. Is it optimal? Well, it's always expanding in the shortest paths first, and so wherever the goal is hiding, it's going to find it by examining no longer paths, so in fact, it is optimal. Cheapest first, first we expand the path of length zero, then the path of length 2. Now there's a path of length 4, path of length 5, path of length 6, a path of length 7, and finally, a path of length 8. And as we've seen, it's guaranteed to find the cheapest path of all, assuming that all the individual step costs are not negative. Depth-first search tries to go as deep as it can first, so it goes 1, 2, 3, then backs up, 4, then backs up, 5, 6, 7. And you can see that it doesn't necessarily find the shortest path of all. Let's say that there were goals in position 5 and in position 3. It would find the longer path to position 3 and find the goal there and would not find the goal in position 5. So, it is not optimal.

## Title 26 - Search Comparison 2 
Given the non-optimality of depth-first search, why would anybody choose to use it? Well, the answer has to do with the storage requirements. Here I've illustrated a state space consisting of a very large or even infinite binary tree. As we go to levels 1, 2, 3, down to level n, the tree gets larger and larger. Now, let's consider the frontier for each of these search algorithms. For breadth-first search, we know a frontier looks like that, and so when we get down to level n, we'll require a storage space of 2 to the n of pass in a breadth-first search. For cheapest first, the frontier is going to be more complicated. It's going to sort of work out this contour of cost, but it's going to have a similar total number of nodes. But for depth-first search, as we go down the tree, we start going down this branch, and then we back up, but at any point, our frontier is only going to have n nodes rather than 2 to the n nodes, so that's a substantial savings for depth-first search. Now, of course, if we're also keeping track of the explored set, then we don't get that much savings. But without the explored set, depth-first search has a huge advantage in terms of space saved. One more property of the algorithms to consider is the property of completeness, meaning if there is a goal somewhere, will the algorithm find it? So, let's move from very large trees to infinite trees, and let's say that there's some goal hidden somewhere deep down in that tree. And the question is, are each of these algorithms complete? That is, are they guaranteed to find a path to the goal? Mark off the check boxes for the algorithms that you believe are complete in this sense.

## Title 27 - Search Comparison 3 
The answer is that breadth-first search is complete, so even if the tree is infinite, if the goal is placed at any finite level, eventually, we're going to march down and find that goal. Same with cheapest first. No matter where the goal is, if it has a finite cost, eventually, we're going to go down and find it. But not so for depth-first search. If there's an infinite path, depth-first search will keep following that, so it will keep going down and down and down along this path and never get to the path that the goal consists of and never get to the path on which the goal sits. So, depth-first search is not complete.

## Title 28 - More On Uniform Cost 
Let's try to understand a little better how uniform cost search works. We start at a start state, and then we start expanding out from there looking at different paths, and what we end of doing is expanding in terms of contours like on a topological map, where first we span out to a certain distance, then to a farther distance, and then to a farther distance. Now at some point we meet up with a goal. Let's say the goal is here. Now we found a path from the start to the goal. But notice that the search really wasn't directed at any way towards the goal. It was expanding out everywhere in the space and depending on where the goal is, we should expect to have to explore half the space, on average, before we find the goal. If the space is small, that can be fine, but when spaces are large, that won't get us to the goal fast enough. Unfortunately, there is really nothing we can do, with what we know, to do better than that, and so if we want to improve, if we want to be able to find the goal faster, we're going to have to add more knowledge. The type of knowledge that is proven most useful in search is an estimate of the distance from the start state to the goal. So let's say we're dealing with a route-finding problem, and we can move in any direction--up or down, right or left-- and we'll take as our estimate, the straight line distance between a state and a goal, and we'll try to use that estimate to find our way to the goal fastest. Now an algorithm called greedy best-first search does exactly that. It expands first the path that's closest to the goal according to the estimate. So what do the contours look like in this approach? Well, we start here, and then we look at all the neighboring states, and the ones that appear to be closest to the goal we would expand first. So we'd start expanding like this and like this and like this and like this and that would lead us directly to the goal. So now instead of exploring whole circles that go out everywhere with a certain space, our search is directed towards the goal. In this case it gets us immediately towards the goal, but that won't always be the case if there are obstacles along the way. Consider this search space. We have a start state and a goal, and there's an impassable barrier. Now greedy best-first search will start expanding out as before, trying to get towards the goal, and when it reaches the barrier, what will it do next? Well, it will try to increase along a path that's getting closer and closer to the goal. So it won't consider going back this way which is farther from the goal. Rather it will continue expanding out along these lines which always get closer and closer to the goal, and eventually it will find its way towards the goal. So it does find a path, and it does it by expanding a small number of nodes, but it's willing to accept a path which is longer than other paths. Now if we explored in the other direction, we could have found a much simpler path, a much shorter path, by just popping over the barrier, and then going directly to the goal. but greedy best-first search wouldn't have done that because that would have involved getting to this point, which is this distance to the goal, and then considering states which were farther from the goal. What we would really like is an algorithm that combines the best parts of greedy search which explores a small number of nodes in many cases and uniform cost search which is guaranteed to find a shortest path. We'll show how to do that next using an algorithm called the A-star algorithm.

## Title 29 - A_ Search 
[Male narrator] A* Search works by always expanding the path that has a minimum value of the function f which is defined as a sum of the g + h components. Now, the function g of a path is just the path cost, and the function h of a path is equal to the h value of the state, which is the final state of the path, which is equal to the estimated distance to the goal. Here's an example of how A* works. Suppose we found this path through the state's base to a state x and we're trying to give a measure to the value of this path. The measure f is a sum of g, the path cost so far, and h, which is the estimated distance that the path will take to complete its path to the goal. Now, minimizing g helps us keep the path short and minimizing h helps us keep focused on finding the goal and the result is a search strategy that is the best possible in the sense that it finds the shortest length path while expanding the minimum number of paths possible. It could be called "best estimated total path cost first," but the name A* is traditional. Now let's go back to Romania and apply the A* algorithm and we're going to use a heuristic, which is a straight line distance between a state and the goal. The goal, again, is Bucharest, and so the distance from Bucharest to Bucharest is, of course, 0. And for all the other states, I've written in red the straight line distance. For example, straight across like that. Now, I should say that all the roads here I've drawn as straight lines, but actually, roads are going to be curved to some degree, so the actual distance along the roads is going to be longer than the straight line distance. Now, we start out as usual--we'll start in Arad as a start state-- and we'll expand out Arad and so we'll add 3 paths and the evaluation function, f, will be the sum of the path length, which is given in black, and the estimated distance, which is given in red. And so the path length from this path will be 140+253 or 393; for this path, 75+374, or 449; and for this path, 118+329, or 447. And now, the question is out of all the paths that are on the frontier, which path would we expand next under the A* algorithm?

## Title 30 - A_ Search Solution 
The answer is that we select this path first--the one from Arad to Sibiu-- because it has the smallest value--393--of the sum f=g+h.

## Title 31 - A Search 1 
Let's go ahead and expand this node now. So we're going to add 3 paths. This one has a path cost of 291 and an estimated distance to the goal of 380, for a total of 671. This one has a path cost of 239 and an estimated distance of 176, for a total of 415. And the final one is 220+193=413. And now the question is which state to we expand next?

## Title 32 - A Search 1 Solution 
The answer is we expand this path next because its total, 413, is less than all the other ones on the front tier-- although only slightly less than the 415 for this path.

## Title 33 - A_ Search 2 
So we expand this node, giving us 2 more paths-- this one with an f-value of 417, and this one with an f-value of 526. The question again--which path are we going to expand next?

## Title 34 - A_ Search 2 Solution 
And the answer is that we expand this path, Fagaras, next, because its f-total, 415, is less than all the other paths in the front tier.

## Title 35 - A_ Search 3 
Now we expand Fagaras and we get a path that reaches the goal and it has a path length of 450 and an estimated distance of 0 for a total f value of 450, and now the question is: What do we do next? Click here if you think we're at the end of the algorithm and we don't need to expand next or click on the node that you think we will expand next.

## Title 36 - A_ Search 3 Solution 
The answer is that we're not done yet, because the algorithm works by doing the goal test, when we take a path off the front tier, not when we put a path on the front tier. Instead, we just continue in the normal way and choose the node on the front tier which has the lowest value. That would be this one--the path through Pitesti, with a total of 417.

## Title 37 - A_ Search 4 
So let's expand the node at Pitesti. We have to go down this direction, up, then we reach a path we've seen before, and we go in this direction. Now we reach Bucharest, which is the goal, and the h value is going to be 0 because we're at the goal, and the g value works out to 418. Again, we don't stop here just because we put a path onto the front tier, we put it there, we don't apply the goal test next, but, now we go back to the front tier, and it turns out that this 418 is the lowest-cost path on the front tier. So now we pull it off, do the goal test, and now we found our path to the goal, and it is, in fact, the shortest possible path. In this case, A-star was able to find the lowest-cost path. Now the question that you'll have to think about, because we haven't explained it yet, is whether A-star will always do this. Answer yes if you think A-star will always find the shortest cost path, or answer no if you think it depends on the particular problem given, or answer no if you think it depends on the particular heuristic estimate function, h.

## Title 38 - A Search 5 
The answer is that it depends on the h function. A-star will find the lowest-cost path if the h function for a state is less than the true cost of the path to the goal through that state. In other words, we want the h to never overestimate the distance to the goal. We also say that h is optimistic. Another way of stating that is that h is admissible, meaning is it admissible to use it to find the lowest-cost path. Think of all of these of being the same way of stating the conditions under which A-star finds the lowest-cost path.

## Title 39 - Optimistic Heuristic 
Here we give you an intuition as to why an optimistic heuristic function, h, finds the lowest-cost path. When A-star ends, it returns a path, p, with estimated cost, c. It turns out that c is also the actual cost, because at the goal the h component is 0, and so the path cost is the total cost as estimated by the function. Now, all the paths on the front tier have an estimated cost that's greater than c, and we know that because the front tier is explored in cheapest-first order. If h is optimistic, then the estimated cost is less than the true cost, so the path p must have a cost that's less than the true cost of any of the paths on the front tier. Any paths that go beyond the front tier must have a cost that's greater than that because we agree that the step cost is always 0 or more. So that means that this path, p, must be the minimal cost path. Now, this argument, I should say, only goes through as is for tree search. For graph search the argument is slightly more complicated, but the general intuitions hold the same.

## Title 40 - State Spaces 
So far we've looked at the state space of cities in Romania-- a 2-dimensional, physical space. But the technology for problem solving through search can deal with many types of state spaces, dealing with abstract properties, not just x-y position in a plane. Here I introduce another state space--the vacuum world. It's a very simple world in which there are only 2 positions as opposed to the many positions in the Romania state space. But there are additional properties to deal with as well. The robot vacuum cleaner can be in either of the 2 conditions, but as well as that each of the positions can either have dirt in it or not have dirt in it. Now the question is to represent this as a state space how many states do we need? The number of states can fill in this box here.

## Title 41 - State Spaces 1 
And the answer is there are 8 states. There are 2 physical states that the robot vacuum cleaner can be in-- either in state A or in state B. But in addition to that, there are states about how the world is as well as where the robot is in the world. So state A can be dirty or not. That's 2 possibilities. And B can be dirty or not. That's 2 more possibilities. We multiply those together. We get 8 possible states.

## Title 42 - State Spaces 2 
Here is a diagram of the state space for the vacuum world. Note that there are 8 states, and we have the actions connecting the states just as we did in the Romania problem. Now let's look at a path through this state. Let's say we start out in this position, and then we apply the action of moving right. Then we end up in a position where the state of the world looks the same, except the robot has moved from position 'A' to position 'B'. Now if we turn on the sucking action, then we end up in a state where the robot is in the same position but that position is no longer dirty. Let's take this very simple vacuum world and make a slightly more complicated one. First, we'll say that the robot has a power switch, which can be in one of three conditions: on, off, or sleep. Next, we'll say that the robot has a dirt-sensing camera, and that camera can either be on or off. Third, this is the deluxe model of robot in which the brushes that clean up the dust can be set at 1 of 5 different heights to be appropriate for whatever level of carpeting you have. Finally, rather that just having the 2 positions, we'll extend that out and have 10 positions. Now the question is how many states are in this state space?

## Title 43 - State Spaces 3 
The answer is that the number of states is the cross product of the numbers of all the variables, since they're each independent, and any combination can occur. For the power we have 3 possible positions. The camera has 2. The brush height has 5. The dirt has 2 for each of the 10 positions. That's 2^10 or 1024. Then the robot's position can be any of those 10 positions as well. That works out to 307,200 states in the state space. Notice how a fairly trivial problem-- we're only modeling a few variables and only 10 positions-- works out to a large number of state spaces. That's why we need efficient algorithms for searching through states spaces.

## Title 44 - Sliding Blocks Puzzle 
I want to introduce one more problem that can be solved with search techniques. This is a sliding blocks puzzle, called a 15 puzzle. You may have seen something like this. So there are a bunch of little squares or blocks or tiles and you can slide them around. and the goal is to get into a certain configuration. So we'll say that this is the goal state, where the numbers 1-15 are in order left to right, top to bottom. The starting state would be some state where all the positions are messed up. Now the question is: Can we come up with a good heuristic for this? Let's examine that as a way of thinking about where heuristics come from. The first heuristic we're going to consider we'll call h1, and that is equal to the number of misplaced blocks. So here 10 and 11 are misplaced because they should be there and there, respectively, 12 is in the right place, 13 is in the right place, and 14 and 15 are misplaced. That's a total of 4 misplaced blocks. The 2nd heuristic, h2, is equal to the sum of the distances that each block would have to move to get to the right position. For this position, 10 would have to move 1 space to get to the right position, 11 would have to move 1, so that's a total of 2 so far, 13 is in the right place, 14 is 1 displaced, and 15 is 1 displaced, so that would also be a total of 4. Now, the question is: Which, if any, of these heuristics are admissible? Check the boxes next to the heuristics that you think are admissible.

## Title 45 - Sliding Blocks Puzzle 1 
H1 is admissible, because every tile that's in the wrong position must be moved at least once to get into the right position. So h1 never overestimates. How about h2? H2 is also admissible, because every tile in the wrong position can be moved closer to the correct position no faster than 1 space per move. Therefore, both are admissible. But notice that h2 is always greater than or equal to h1. That means that, with the exception of breaking ties, an A* search using h2 will always expand fewer paths than one using h1

## Title 46 - Sliding Blocks Puzzle 2 
Now, we're trying to build an artificial intelligence that can solve problems like this all on its own. You can see that the search algorithms do a great job of finding solutions to problems like this. But, you might complain that in order for the search algorithms to work, we had to provide it with a heurstic function. A heurstic function came from the outside. You might think that coming up with a good heurstic function is really where all the intelligence is. So, a problem solver that uses an heurstic function given to it really isn't intelligent at all. So let's think about where the intelligence could come from and can we automatically come up with good heurstic functions. I'm going to sketch a description of a program that can automatically come up with good heurstics given a description of a problem. Suppose this program is given a description of the sliding blocks puzzle where we say that a block can move from square A to square B if A is adjacent to B and B is blank. Now, imagine that we try to loosen this restriction. We cross out "B is blank," and then we get the rule a block can move from A to B if A is adjacent to B, and that's equal to our heurstic h2 because a block can move anywhere to an adjacent state. Now, we could also cross out the other part of the rule, and we now get "a block can move from any square A to any square B regardless of any condition. That gives us heurstic h1. So we see that both of our heurstics can be derived from a simple mechanical manipulation of the formal description of the problem. Once we've generated automatically these candidate heuristics, another way to come up with a good heurstic is to say that a new heurstic, h, is equal to the maximum of h1 and h2, and that's guaranteed to be admissible as long as h1 and h2 are admissible because it still never overestimates, and it's guaranteed to be better because its getting closer to the true value. The only problem with combining multiple heuristics like this is that there is some cause to compute the heuristic and it could take longer to compute even if we end up expanding pure paths. Crossing out parts of the rules like this is called "generating a relaxed problem." What we've done is we've taken the original problem, where it's hard to move squares around, and made it easier by relaxing one of the constraints. You can see that as adding new links in the state space, so if we have a state space in which there are only particular links, by relaxing the problem it's as if we are adding new operators that traverse the state in new ways. So adding new operators only makes the problem easier, and thus never overestimates, and thus is admissible.

## Title 47 - Problems With Search 
We've seen what search can do for problem solving. It can find the lowest-cost path to a goal, and it can do that in a way in which we never generate more paths than we have to. We can find the optimal number of paths to generate, and we can do that with a heuristic function that we generate on our own by relaxing the existing problem definition. But let's be clear on what search can't do. All the solutions that we have found consist of a fixed sequence of actions. In other words, the agent Hirin Arad, thinks, comes up with a plan that it wants to execute and then essentially closes his eyes and starts driving, never considering along the way if something has gone wrong. That works fine for this type of problem, but it only works when we satisfy the following conditions. [Problem solving works when:] Problem-solving technology works when the following set of conditions is true: First, the domain must be fully observable. In other words, we must be able to see what initial state we start out with. Second, the domain must be known. That is, we have to know the set of available actions to us. Third, the domain must be discrete. There must be a finite number of actions to chose from. Fourth, the domain must be deterministic. We have to know the result of taking an action. Finally, the domain must be static. There must be nothing else in the world that can change the world except our own actions. If all these conditions are true, then we can search for a plan which solves the problem and is guaranteed to work. In later units, we will see what to do if any of these conditions fail to hold.

## Title 48 - A Note On Implementation 
Our description of the algorithm has talked about paths in the state space. I want to say a little bit now about how to implement that in terms of a computer algorithm. We talk about paths, but we want to implement that in some ways. In the implementation we talk about nodes. A node is a data structure, and it has four fields. The state field indicates the state at the end of the path. The action was the action it took to get there. The cost is the total cost, and the parent is a pointer to another node. In this case, the node that has state "S", and it will have a parent which points to the node that has state "A", and that will have a parent pointer that's null. So we have a linked list of nodes representing the path. We'll use the word "path" for the abstract idea, and the word "node" for the representation in the computer memory. But otherwise, you can think of those two terms as being synonyms, because they're in a one-to-one correspondence. Now there are two main data structures that deal with nodes. We have the "frontier" and we have the "explored" list. Let's talk about how to implement them. In the frontier the operations we have to deal with are removing the best item from the frontier and adding in new ones. And that suggests we should implement it as a priority queue, which knows how to keep track of the best items in proper order. But we also need to have an additional operation of a membership test as a new item in the frontier. And that suggests representing it as a set, which can be built from a hash table or a tree. So the most efficient implementations of search actually have both representations. The explored set, on the other hand, is easier. All we have to do there is be able to add new members and check for membership. So we represent that as a single set, which again can be done with either a hash table or tree.

## Title 49 - Challenge Question Revisited
Searching is one of the most fundamental methods of solving problems in AI. We've now seen depth-first search, breadth-first search, uniform cost search in a star. In the book, we've read about iterative deepening and bidirectional search and the space and time advantages of these different search methods. Remember Sally from the Carter Center? Let's use the knowledge from the section to look at the problem more thoroughly. Depth-first search is non-viable here. If we were unlucky, we might cross a country several times in just our first branch. Breadth-first search, or our uniform cost search would work, but we'd have to do three searches, one from Little 5 Points to Scottdale, one from Scottdale to Buckhead, and one from Buckhead to Little 5 Points. We would then determine the shortest two of the three paths. And so, the pulling station that is common between those two paths is a second pulling station we would visit. We could then start at either of the other two cities. The problem is that this method revisits some nodes repeatedly in the three searches, and the search would range much farther than is necessary. A bi-directional search would help considerably. Instead of each of the three searches requiring B to the D nodes, it would be done in B to the D over two nodes. But we would still revisit a lot of the nodes in the triangle between the three cities. Instead, what if we start up the search at all the locations simultaneously? Growing them out until the first two connect, and then continuing all until the third location connects somewhere with the first two. That would avoid the repeated node visits. The question is, can we modify this uninformed tridirectional search into a tridirectional a star search? The key is figuring out an admissible heuristic that takes into account two potential goals at the same time. Give it some thought, see if you can come up with one

## Title 50 - Peter's take on AI
I've been asking this question of all the AI researchers I interview. What is your definition of artificial intelligence? &gt;&gt; To me, AI is programming a computer to do the right thing when you don't know for sure what the right thing is. &gt;&gt; So Peter, you've written the most widely used AI textbook in history. Can you tell us about that process and what led to its success? Well, around 1990, I remember the AI faculty at Berkeley, we all went to lunch together. And we were lamenting that none of the textbooks available at that time were quite what we wanted. And we should write a new text, we said, but it didn't happen. I ended up leaving Berkeley, and I went into industry, first with Sun Microsystems. About a year later, I saw Stuart Russell at a conference. And I said, well, you guys must be almost done with that book you were always talking about. And he said, no, they hadn't done anything. So I said, well, the two of us should just go ahead and do it, and we did. I think the book was successful because of a fortuitous accident of timing. We happened to come along just as AI was making this transition from Boolean logic to probability, and from handcrafted knowledge to machine learning. And so we were able to capture that in our book in a way that wasn't in previous books. &gt;&gt; Your intro to AI course has had over 160,000 students, and led to the founding of Udacity. Why did you and Sebastian decide to do the course in the first place? &gt;&gt; Well, we taught the intro to AI course at Stanford in 2010, a regular course in the classroom. And then in winter 2011, I guess they couldn't come up with anyone else to teach the course, so they asked us to do it again, and we agreed. But since we had just done it, we decided we wanted to do something different this time. And Sebastian suggested opening the course up to the world. I thought, that's a great idea. Maybe we might get 1,000 students to sign up. Well, I was off by two orders of magnitude. &gt;&gt; Wow. What lessons did you learn from being the head of search quality for Google in the early days? &gt;&gt; I think one important lesson is, pay attention to the data. Understand what's out there, what you're doing, and how well you're doing. Keep experimenting and measuring how well you're doing, and that way, you can improve. And two, pay attention to people. I think we did a good job at building a high-quality search engine. But it all comes down to having great people and keeping them happy and productive. &gt;&gt; So Peter, we both love Lisp as a programming language. What do you think that makes it special? &gt;&gt; I would say that today, Lisp is no longer really special. But to me, that means that Lisp won. It influenced all these other languages into adopting the key features. What things are they? Well, a rich collection of data types, a read-eval-print loop, the notion of debugging a running program and being able to easily inspect the data. Preference for lots of functions, many without side effects, object orientation and inheritance if you want it. All these things were unique to Lisp, but now they're in most languages. &gt;&gt; I think the one thing that hasn't caught on in other languages was the ability to define rich macros. Some languages have a limited macro capability. Lisp really went much farther. But that hasn't moved into current date languages yet. &gt;&gt; So when we first talked about my preparing this course, you convinced me to use Python for the assignments, a language I didn't know at the time. Why do you think Python is good for teaching this course? &gt;&gt; Well, when Stuart and I wrote the book, we didn't want to impose a programming language choice on the reader or the instructor. And so we just used a pseudo-code in the algorithms in the book. We made up a format, and then we provided Lisp implementations for the code online. Over time, fewer and fewer students knew Lisp, so I decided I would have to rewrite the code in a language. And I wanted one that would be as close to the pseudo-code as possible, popular enough that many students would know it. And easy enough so that if a student didn't know it, they'd still be able to understand it and pick it up quickly. And so I surveyed all the languages, and Python seemed to be by far the best choice. Somehow, Guido and we were in sync. And we thought of the same pseudo-code, more or less, as he thought of as a language. And so that worked out well. &gt;&gt; Cool, thank you. 
