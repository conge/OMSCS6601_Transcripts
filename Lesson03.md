# Lesson 3

## Title 01 - Traveling Salesman Problem
You've heard me say, do the stupid thing first and only add intelligence when necessary. It turns out that there are a whole class of problems where just adding a little bit of intelligence and iteratively improving the solution gets you very close to an optimal solution. &gt;&gt; Like what? &gt;&gt; One of the classic ones is the traveling salesman problem. Imagine you're a salesman. &gt;&gt; That might be nice. I'd probably make more money than what you're paying me. [SOUND] As I was saying, imagine you're a salesman and you have five cities you want to visit. You can start in any city, but you have to fly to all of them before your tour is over. And you have to come back to the starting city in the end. What is the most efficient order of flights to minimize overall distance flown. This problem sounds NP hard. &gt;&gt; What do you mean? &gt;&gt; Well, NP actually means non-deterministic polynomial time. But people commonly talk about non-polynomial algorithm as all being about as hard as each other or NP hard. This problem seems exponentially difficult in the number of cities considered and so it seems just as bad as our game playing or search problems. &gt;&gt; Precisely. But like with our other problems, we're going to figure out tricks to help solve the problem efficiently. First we're going to connect the cities randomly. &gt;&gt; How about like this? &gt;&gt; Great. Next we are going to look at any place where the paths cross. We'll show them in red here. &gt;&gt; I get it. We need to revise the figure to uncross each situation. That should reduce the distance traveled. &gt;&gt; Yep, and for large problems we can do the process iteratively until there are no crossed paths. You can do this simple process with thousands of cities and get a result that is within 1% of the optimal solution. &gt;&gt; But that seems like a hack. How does this technique generalize? &gt;&gt; That is the subject of this section, it will improve in algorithms. Let's start out with our challenge question.

## Title 02 - 4-Queens  1
Can we get another example of an iterative improvement problem? &gt;&gt; Sure. Here's one we'll use throughout this lesson. It's called N Queens. Basically, the puzzle is to place n queens so they cannot attack each other on an n by n chess board. &gt;&gt; In other words, no queen can be on the same horizontal row, vertical column or diagonal. &gt;&gt; Okay, so here's an example board of 4 queens. I just put down the queens randomly and there are five ways that the queens could attack each other. &gt;&gt; So our goal is to get the number of attacks down to 0? &gt;&gt; Yep, and we have four dimensions in which to move. Specifically, we can move queen in its column to minimize the number of attacks. &gt;&gt; Well, there are two queens that have three attacks. It seems like a good idea to move one of those first. If we move the second queen to the top row, we could eliminate three of the five attacks. &gt;&gt; And that leaves only two attacks left. &gt;&gt; It seems like we should always start from the queen with the most attacks. This reminds me of the most constrained variable heuristic in constraint satisfaction problems, but we'll cover that topic in the next lesson. &gt;&gt; Yep, and taking that approach, the next queen we should work on is the third one which is involved in two attacks. If we move it to the bottom row, then we will have no more attacks and we've solved the problem. &gt;&gt; That's surprisingly effective. We've solved the problem in just two moves, whereas with simple search, we would have had many more steps. &gt;&gt; Exactly. It is an example of trying the stupid thing first and then adding intelligence until we solve the problem. However, 4 queens seemed too easy. Let's try 5 queens and see if it gets harder.

## Title 03 - 5-Queens Quiz
Can we solve the 5-queens problem? Check the boxes for spaces that will contain a queen such that no two are attacking each other. There are multiple solutions here, so just put down one.

## Title 04 - 5-Queens Quiz Solution
Here is one possible solution. [BLANK_AUDIO]

## Title 05 - n-Queens Heuristic Function
Let's be a little more formal about the n-Queens problems. Given a current board we want to make the one move that can most greatly improve the situation. In fact to simplify the discussion let's constrain ourselves to moving a single queen up or down within its column. Then we keep iterating until we reach our goal of having zero attacks. &gt;&gt; Given this board which currently has 17 attacking pairs. The best we can do with a single move is to get the number of attacks down to 12. For example, if you move this queen here, you reduce the number of conflicts to 12. &gt;&gt; That's right. But, how do we figure out which one to take? Do we just select it randomly? &gt;&gt; Seemed as good an idea as any. &gt;&gt; Okay so we can continue to iterate, hopefully reducing the number of attacks with each move. But what if we get into a situation where no move decreases our number of attacks?

## Title 06 - n-Queens  Local Minima
You mean like this board? &gt;&gt; Hold on, I have to think about this one for a second. Okay, I see that there's only one current attack on this diagonal. So when I move one of these queens I should be done. &gt;&gt; Sure, why not try it out? &gt;&gt; Okay, well let's work with the queen in the fourth column first. I can't put it in the topmost row, because it gets attacked by the same queen as well as its diagonal neighbor. With the next row down, it's attacked both here and here. The next row down is attacked both here and here. Hold it, there is no place on this board I can put it without being attacked horizontally. Except where I had it originally, in fact, every possible spot makes the problem worse. We'll cause more attacks. &gt;&gt; Is there anything else we could do? &gt;&gt; We'll have the same problem with the queen in the second to the last column. Moving it down one row leads to two attacks, here and here. Going to the next row causes two attacks, as well. Every single move with this queen gets more attacks, as well. And moving any of the other six queens will also cause an increase of attacks because they will be attacked horizontally, plus we will still have the original attack. For example, look at this left most queen, every row, except for the one it's in, currently will have another attack. &gt;&gt; Does that mean the problem is unsolvable? &gt;&gt; No, I know the problem is solvable, but I seem to be stuck. No one move will improve my situation, this puzzle hurts my brain. &gt;&gt; Maybe we can use a simpler problem to figure out a solution and then come back to this one later. &gt;&gt; That's a great idea.

## Title 07 - Hill Climbing  1
The n queens problem was multidimensional. And we were trying to minimize the number of attacks. Here for convenience we'll make our goal to maximize this value. And we can only move left or right along the x-axis instead of having so many different pieces to move.

## Title 08 - Local Maximum
Suppose I start here. I look to the left and it goes down. I look to the right and it's going up. So I'll take a step in that direction. &gt;&gt; And what do you do when you reach the top? &gt;&gt; Well we will get stuck, just like we did with the end queens problem. There's no immediate step that I can do that has a positive gradient. Everything is negative, that's a bummer. &gt;&gt; Is that a problem, didn't we want to get to a maximum? &gt;&gt; We actually wanted to get to the global maximum. Looking at the graph, we can see that there's actually a taller peak over here. &gt;&gt; I see, but our computer agent doesn't know that. It can only see what's one step away from it, right? So how are you going to get unstuck? &gt;&gt; But what if I just start again somewhere else?

## Title 09 - Random Restart.srt

## Title 10 - Hill Climbing Quiz  1
We are going to consider multiple starting points for the hill climbing algorithm in this quiz. By convention, we'll call these particles. For each of these particles, tell us their value assuming your algorithm has a step size of one and that it stops when no positive gradient is found.

## Title 11 - Hill Climbing Quiz Solution
Particle 1 starts at x=1 and follows a positive gradient up, but then it gets stuck at the shoulder where y=2. Particle 2 starts at x=8 and follows a positive gradient up, but it gets stuck at this local maximum at y=6. Particle 3 starts at x=9 and follows the positive gradient in the right direction, but again it gets stuck at a local maximum, at y=8. Particle 4 starts at x=14 and also goes to the same local maximum. Particle 5 starts at x=20 and follows the positive gradient up towards the global maximum at y=12.

## Title 12 - Step Size Too Small
Does hill climbing have any other problems we should worry about? &gt;&gt; Suppose we start at the left edge of the graph, using small steps we gradually climb higher until we get to the shoulder here. However, if we stop when we no longer see a way to improve we can get stuck at this shoulder. &gt;&gt; So you mean we get to this point and if the step size is small enough we never see the sudden increase in gradient here? &gt;&gt; Yup. For flat areas how do we know which way to travel? If we just choose a direction randomly we can wander on this plateau for a while. The code needs to return a result at some point. So how many times in a row should we allow a zero improvement in score before we stop? We could just keep going in the direction of the last positive gradient. But with a small enough step size, we can easily think we are not improving and the algorithm should stop.

## Title 13 - Step Size Too Large
Well, then why not just keep the step size large? &gt;&gt; A couple of reasons. First, with a really large step size, we can miss sharp hills completely. Going back to our first starting place if my step size is this big, I would skip the hill I intended to take and would start start going up the next hill instead. &gt;&gt; Well, that seems obvious. But you said there were a couple of reasons, what's the other one? &gt;&gt; In certain situations, the algorithm could get into an infinite loop and never terminate. For example, look at the hill with the global maximum. If I start to the left of it, above the shoulder, and have a step size this big, I'll skip to the other side of the hill. Now the gradient is going to go back the other way. So I step back in the direction in which I came. End up close to where I started. The algorithm can oscillate and not converge on the answer. &gt;&gt; But it should be easy to check for that in our code. Maybe if we see oscillation we could just do smaller steps. &gt;&gt; You've hit upon a key idea. We can start with a large step size and decrease it over time to better ensure that we reach the global maximum. However, we'll get the same result with something called simulated annealing which is one of the big concepts in this course. &gt;&gt; Before we continue I'd like to experiment a bit with these hill climbing ideas.

## Title 14 - Hill Climbing Quiz 2
Now assume your algorithm has a step size of 2. It still stops when no positive gradient is found. What value would these particles return? Particle 1 starts at x equal to 7. Particle 2 starts at x equal to 8, and particle 3 starts at x equal to 10. Select their appropriate answer for each particle from the choices given here.

## Title 15 - Hill Climbing Quiz 2 Solution
Particle 1 starts at 7, and with a step size of two, it reaches the peak at 10 with one step. Particle 2 starts at 8, it goes towards the peak, but then it oscillates around the peak because the step size is too large. Particle 3 starts at 10, it follows this upward gradient, but then it gets stuck at this plateau.

## Title 16 - Annealing
We've been trying to emphasise algorithms that are of a particular importance. &gt;&gt; A star and alpha-beta pruning on the minimax algorithm are some examples. &gt;&gt; Now we're going to introduce another key concept, simulated annealing. &gt;&gt; Hold on. Simulated annealing? That implies that there's real annealing too. &gt;&gt; That's right. We're going to steal some ideas from the physicists for this section. First let's talk about energy minimization. When external conditions allow molecules to be mobile, and then mobility of the molecules slowly reduces, the molecules then arrange themselves into the lowest energy configuration. Often these conditions result in regular patterns. &gt;&gt; You mean like mud cracks. &gt;&gt; Yep, that's a common example where the decreasing amount of water in the mud reduces the molecules and mobility over time. And the mud cracks into regular patterns. &gt;&gt; And we see similar structures in honeycombs. Honeybees try to optimize their storage space and minimize the building materials for the structure that their building. &gt;&gt; That's an example of minimization done by design. But the same effect can happen in the earth as erupted lava slowly cools and rocks form. Here's an example of columnar basalt from the Devils Postpile in California. As the rock cools it shrinks and cracks into hexagonal lattice which is a minimal energy configuration. &gt;&gt; Iron molecules have similar low energy states into which they can pack. They can form a lattice with other molecules like carbon. Some of these lattices are harder or softer than others. For certain applications like sword making a mixture of hardness and ductility is needed. &gt;&gt; Sword makers realize the need to heat and cool iron repeatedly to get iron molecules to align in the desired lattice structures to make swords hard but not brittle. They heated the iron above the temperature where the atoms could move about and form new structures with carbon and other atoms. Then they cooled the iron to preserve the types of lattice structures they wanted. This annealing process was repeated according to closely guarded formulas and times to get the desired properties. The best steel-makers were treasured. &gt;&gt; Looking at the edge of the blade of a finely made sword or cooking knife shows a process of heating and recrystallization. But how does annealing help our problem? &gt;&gt; We are going to use the idea of heating and cooling to help us get out of the local minima on the way to finding global minimum. For us, high temperature equates to more randomness. And gradual cooling will decrease the randomness until we converge on a solution.

## Title 17 - Simulated Annealing  1
Here is the version of the algorithm we're going to use. Just like hill climbing, we're going to iterate with simulated annealing, looking for points close to our current position that might have an improved value. However, we're going to select our next position randomly from the points in the region near us. If a new position is better than our current position, we're going to take it. However, if it isn't better, we're still going to take it with a probability of e to the delta E divided by T, where T is our temperature. &gt;&gt; So we start with a high T, do we change it later on? &gt;&gt; Yes, in the algorithm we have a schedule for what the temperature should be. And we'll start with T being very high. And when T is high, say close to infinity, delta E over T goes to 0. No matter what the delta E is, even if it's negative. So e to 0 is 1. So in the beginning, we have a lot of random motion as we take all the random positions offered to us. No matter how bad the new position is. &gt;&gt; I see, just like in real annealing, when the temperature is very high, the particle's jumping around a lot in the beginning. So that if the particle gets stuck in a local maximum instead, it has the ability to leave that peak, ignoring that it is going the wrong way and instead end up at a different part of the graph. Because the temperature is high, the next point that's fixed randomly can make it move down the slope. Given enough time the randomness will ensure we get off this particular hill and hit the hill with the global maximum instead. &gt;&gt; Precisely. &gt;&gt; But how does it converge? Won't we continue doing this random walk forever? &gt;&gt; Well, let's first take a look at the opposite situation were T is near 0. We never want T to actually be 0 because that will give us an undefined answer. So for illustration purposes let's say T is equal to 0.01. We already said that if the purposed new random position improves our score, we're going to take it. If it stays the same delta E is 0. Or if it gets worse, delta E is negative. Let's say delta E here is -1. Now we have e to the -1 over 0.01. Which is the same as e to the -100. Which a very small number. So we have almost no chance of taking that suggested new random position. Instead, we'll keep generating new random positions until we get one that improves e. &gt;&gt; Then, when T is small, we basically have normal hill climbing &gt;&gt; Correct. &gt;&gt; So we'll just slowly change T from very large, where we're going to move over the graph randomly, to very small, where we climb to the nearest peak. &gt;&gt; Yep. And if we happen to get stuck at a plateau along the way, like here, delta E is 0. But that makes the equation e to the 0 again. So that the algorithm will take the new random position instead. &gt;&gt; That will happen no matter what T is. &gt;&gt; And eventually, we'll random walk off the plateau, back to someplace where there's a positive gradient and continue going up to the maximum. &gt;&gt; Exactly, the great thing about simulated annealing is that it's guaranteed to converge to the global maximum if we start T high and decrease it slowly enough.

## Title 18 - Simulated Simulated Annealing
One of my undergraduate students, Juliet, has created a nice simulation of simulated kneeling using Raptor prototyping. &gt;&gt; What does it do? &gt;&gt; Well we put in a ball that represents our particle, and we shake the box really hard. [SOUND] &gt;&gt; And as you slowly decrease the amount of shaking, the ball falls into the deepest well and stays there. But I have to be careful, to slowly decrease the temperature, or else it will not work. &gt;&gt; You all seem to work very hard for few seconds of demonstration. &gt;&gt; There's definitely a theme in my research.

## Title 19 - Local Beam Search  1
While we're on this topic, I'd like to talk about local beam search, because we'll use it later in the course. &gt;&gt; Okay, it's your show after all. &gt;&gt; The local beam search, instead of just using one position, which I'll call a particle, just because that's how I think of it, will keep track of k particles. At each time frame, we'll look at the randomly generated neighbors of each of these particles and keep the k best ones for the next iteration. If ever any of these particles have reached a goal, we terminate. Now you'd think that this algorithm is just like random restart. &gt;&gt; But it's not, because we are comparing all the neighbors of all the particles to each other, there is information being passed between each position, right? Normal random restart doesn't share any of this information between iterations. &gt;&gt; That's correct. &gt;&gt; Okay, that seems fine. But I've heard you talk about stochastic beam search before as being more useful. How is that different? &gt;&gt; Stochastic beam search is the same thing. But the successors are chosen not just based on their fitness, but some randomness. Which helps ensure we don't get stuck in a local maximum. This idea has some similarity to simulated annealing. As in the class of algorithms to which generic algorithms belongs, which we'll tackle next.

## Title 20 - Representing n-Queens
We're going to use the n-Queens problem again to talk about genetic algorithms, but before we do that, we need to create a convention to represent a given board. &gt;&gt; We'll use the one in the book. Since there can only be one queen in each column, we can encode a board with the number per column that indicates where the queen is. So for this board, the encoding would be 86427531.

## Title 21 - 8-Queens Representation  1
Here's another example board for 8-Queens. Give us a string that represents the position of each piece on this board.

## Title 22 - 8-Queens Representation Solution
Here is the answer. [BLANK_AUDIO]

## Title 23 - Genetic Algorithms
Another specific we need to know before doing our genetic algorithms example is that there are 28 plausible pairs of attacking queens on this eight by eight board. &gt;&gt; How did you figure that out? &gt;&gt; Well, we have eight queens and we want to examine every possible pair that could attack each other. So that it has 8 choose 2 or 8 factorial over 8-2 factorial times 2 factorial. &gt;&gt; That's good to know since our goal with n queens is to reduce the number of attacking pairs of queens to 0, we're going to to find a fitness function for a board to be the maximum number of attacking pairs of queens, which is 28, minus the number of attacking queens for a given board. In the case of eight queens, when the fitness function reaches 28, we'll know we've won. Genetic algorithms is an analogy to natural selection in biology. It uses breeding and mutation to find the optimal answer to a problem. I think it's best to explain genetic algorithms with an example. &gt;&gt; Okay, then let's work on the eight queens problem. To get things started, let's choose four random boards. These four boards represent our gene pool, and we're going to try and breed better boards by combining them in different patterns. &gt;&gt; How do we choose which ones to use? &gt;&gt; Easy, survival of the fittest. We'll evaluate each board according to the fitness function we just described. The top board only had 4 attacking pairs, which means it has a score of 28 minus 4, or 24. The next one had 5 attacking pairs, so its score is 23. The remaining two have scores of 20 and 11 respectively. &gt;&gt; Poor number 4. He will probably never get to have kids. Evolution can be cruel. &gt;&gt; Anyway, from these fitness scores, we will create proportional probabilities for how likely each one is to breed. &gt;&gt; So basically, we're going to add the four scores and normalize each one into a percentage. &gt;&gt; Yep. &gt;&gt; Okay, so 24 plus 23 plus 20 plus 11 is equal to 78. And 24/78 = 31%. So our most fit board has a 31% chance to be chosen as a parent. &gt;&gt; That's right, we'll continue to calculate these percentages for each board. And we get 29%, 26%, and 14%. &gt;&gt; Poor number four. &gt;&gt; Would you stop it? &gt;&gt; [SOUND] &gt;&gt; Okay, now we're going to select four parents to create four new children. To do that, we basically roll a hundred sided die to select the first parent. If it is 1 to 31, we select the first board, and if 32 to 60, the second, and so on. Say we roll a 55. So for the first parent, we get the second board. &gt;&gt; And we roll the 100 sided die again. And for the second parent, I get the first board. &gt;&gt; And we continue this process. The third parent will be the second board. &gt;&gt; Lucky board. &gt;&gt; And for the last parent, we get the third board. &gt;&gt; Poor- &gt;&gt; Don't say it! &gt;&gt; Okay. So for these lucky boards, they are going to produce offspring. How many children will we produce from each set of parents? &gt;&gt; Two. &gt;&gt; Is there any reason for that? &gt;&gt; Well, with genetic algorithms, there are many parameters we can change to try to optimize how quickly we converge to a good result. Let's stick with two here for convenience. &gt;&gt; Okay, so now that we know which parents we have and how many children we will produce, how do we make children? &gt;&gt; That's a process we call crossover. You see, we've selected a point along each pair of parents. We pick this point randomly, and then we create the first child by taking the first part of the first parent and then tack that on to the last part of the second parent. &gt;&gt; This process sounds more like assembling Frankenstein's monster from body parts than breeding, but okay. &gt;&gt; [SOUND] Then for the second child we take the first part of the second parent and add it to the last part of the first parent. We do the same process with the second set of parents here. &gt;&gt; It's alive! &gt;&gt; [LAUGH] ` &gt;&gt; So hopefully, at least one of the children will get the best attributes of the parents. It kind of reminds me of the movie Twins with Arnold Schwarzenegger and Danny Devito. &gt;&gt; You're showing your age. I wasn't born yet when that movie came out. &gt;&gt; Still, it was a good movie. But I actually had a point. What if, like in the supposition in the movie, one child gets all the good aspects of the parents and the other one gets all the bad aspects? &gt;&gt; Well, then in the next generation, the one with less attacking queens will have a higher fitness function and have a higher chance to create children. And the one with more attacking queens will have a lower fitness function and less chance to create children. &gt;&gt; And we hope that by having enough generations, we will eventually evolve an eight queens game board that solves the problem? &gt;&gt; There's one more critical step. But first, let's review crossover more carefully.

## Title 24 - GA Crossover
Okay, let me handle this one. Given a parent, 32752411, which corresponds to a board that looks like this, we've randomly selected the spot between the third and fourth column to be the crossover point. For the first child, we'll take the first part of the first parent, which we'll mark in blue. Looking at the second parent, which is 24748552, we're going to take the last part of it. Let's mark that in yellow. And now, we'll add them together to get a child that is 32748552. That makes more sense to me now that I see it done with the actual game boards.

## Title 25 - GA Mutation  1
But there is one problem. What if there is a critical part of the solution that is in none of the pairings? &gt;&gt; That could be an issue. I can also imagine that a critical piece might be in a board that never gets selected as a parent. Like poor number four here. And that critical piece gets bred out in the early stages and never comes back. How do we handle it? &gt;&gt; More randomness. &gt;&gt; How so? &gt;&gt; Just like we have mutations in biology, we're going to use mutations in genetic algorithms. For each digit I'm going to have a small but significant chance that the digit will mutate into some other digit. In the first child the 5 changes into a 1. The second child does not have any mutations, and the third and fourth children each have one mutation. &gt;&gt; So this mutation step is like occasionally choosing a random direction and simulated annealing. Where the randomness is stochastic beam search. &gt;&gt; Exactly. &gt;&gt; And given enough generations and mutations we expect to get the desired answer. &gt;&gt; Yup. And one of the things often reported in genetic algorithms papers is how many generations are needed until we get a good answer. &gt;&gt; Okay, now that we've seen all the components of genetic algorithms it seems like a good time for a quiz.

## Title 26 - GA Crossover Quiz
Try to apply what you've learned so far to complete this iteration of the genetic algorithm. First, fill in the fitness value or the number of non attacking pairs of queens for each board state in the initial population. Then, choose parents according to their fitness score and fill them in this column. Order them by fitness value where the best is on top. Now cross over the top two selected parents, making sure that they're different individuals. Then produce two children using a four, four split. Finally, fill in the fitness values for the resulting children.

## Title 27 - GA Crossover Quiz Solution
Here's the answer. Note that the children are worse than some of the parents. In progressive generations, we might continue to see this. But with mutation step, we might get closer to the goal. Without the mutation step, we run the risk of never actually reaching the goal.

## Title 28 - Method Similarities
Shelly, is there any idea of reducing the randomness of a time in generic algorithm like say in simulated? &gt;&gt; Well, not limitations that we have shown so far. But the fitness function naturally reduces the randomness as each generation gets closer to the solution. However, we could argue that the method could do better. In fact, there are a lot of publications that talk about tuning, cross over, mutations, number of parents and all the other parameters to optimize genetic algorithms, to converge as quickly as possible. &gt;&gt; In some sense, genetic algorithms are a fancy version of beam search that happens to make a nice analogy with biology. &gt;&gt; Yep, some people call genetic algorithms the second best solution to any problem. But analogies are nice things when learning and using different methods. We used a lot throughout this section. Hill climbing, a kneeling and genetic algorithms are all analogies to some physical or biological process.

## Title 29 - Challenge Question Revisited
Let's revisit the challenge question from the beginning of this section. We asked about which methods enable us to find the maximum point in this graph. We show that we can have problems getting stuck in local maxima or on plateaus where there is not enough local information to determine which way to go to improve the answer, or determine if we have reached the maximum. We used randomness in three methods to get out of this trap. They included random restart, where we start many particles at random positions on the graph, hill climb, and take the best results. Another solution was simulated annealing, where we make an analogy to the process of gradually cooling the temperature of our particle to make it move in progressively less random ways while hill climbing. Until we converge on the global maximum. Finally, we said genetic algorithms could help us. This is where we select positions based on their fitness to breed children that mutate, but eventually converge on the solution. We talked about how a random restart, simulated annealing, and genetic algorithms are combined into the ideas to cast a theme search. Where multiple particles are released on the graph, and their children generate progressively less randomly, eventually converge to the maximum value. We'll see these ideas repeatedly in AI, in techniques like particle filters, pattern recognition with hidden Markov models, and Monte Carlo Markov chains. Keep these ideas in mind whenever you're creating your own algorithms or attacking new problems. Sometimes letting go and embracing randomness is the key to the solution. 
